{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from generator import RoadNetwork, Trajectory\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from models import TemporalGraphTrainer, GTCModel, Traj2VecModel, ModelVariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load required data\n",
    "unmapped_traj = pd.read_csv(\"../../../datasets/trajectories/hanover/temporal/mapped_id_poly_clipped_small_graph.csv\", \";\")\n",
    "traj = Trajectory(\"../../../datasets/trajectories/hanover/temporal/road_segment_map_final_small_graph.csv\", nrows=100000000).generate_TTE_datatset()\n",
    "traj[\"seg_seq\"] = traj[\"seg_seq\"].map(np.array)\n",
    "traj = traj.join(unmapped_traj[[\"start_stamp\", \"end_stamp\", \"id\"]].set_index(\"id\"), on=\"id\", how=\"left\")\n",
    "network = RoadNetwork()\n",
    "network.load_edges(\"../../../osm_data/hanover_temp_small_graph/\")\n",
    "network.gdf_edges.rename(columns={\"speed_limi\": \"speed_limit\", \"highway_en\": \"highway_enc\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folium Heat Map\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "from collections import defaultdict\n",
    "import branca.colormap\n",
    "\n",
    "m = folium.Map(location=[52.37052, 9.73322],\n",
    "                    zoom_start = 8)\n",
    "\n",
    "sub_df[\"coords\"] = sub_df[\"geometry\"].swifter.apply(lambda x: list(x.coords))\n",
    "coords = sub_df.loc[:, \"coords\"].values\n",
    "\n",
    "for line in coords:\n",
    "    data = [(c[1], c[0]) for c in line]\n",
    "    folium.PolyLine(data, color=\"red\", weight=2.5, opacity=0.8).add_to(m)\n",
    "\n",
    "# coords = gdf[\"coords\"].values\n",
    "# for line in coords:\n",
    "#     data = [(c[1], c[0]) for c in line]\n",
    "#     folium.PolyLine(data, color=\"green\", weight=2.5, opacity=0.8).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "#map_porto.save(\"heatmap_gps_points_porto.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seg_seq</th>\n",
       "      <th>travel_time</th>\n",
       "      <th>start_stamp</th>\n",
       "      <th>end_stamp</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>end_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[917, 910, 911, 914, 919, 932, 941, 943, 946, ...</td>\n",
       "      <td>655</td>\n",
       "      <td>2019-09-21 13:18:03</td>\n",
       "      <td>2019-09-21 15:21:49</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19105</td>\n",
       "      <td>[640, 631, 627, 613, 603, 598, 594, 592, 898, ...</td>\n",
       "      <td>1753</td>\n",
       "      <td>2019-11-23 00:14:00</td>\n",
       "      <td>2019-11-23 02:20:02</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19107</td>\n",
       "      <td>[433, 437, 444, 464, 340, 345, 350, 356, 360, ...</td>\n",
       "      <td>313</td>\n",
       "      <td>2019-11-23 00:14:00</td>\n",
       "      <td>2019-11-23 02:20:02</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[179, 170, 169, 566, 557, 547, 543, 535]</td>\n",
       "      <td>120</td>\n",
       "      <td>2019-11-05 11:01:07</td>\n",
       "      <td>2019-11-05 13:17:07</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1010, 1011, 1014]</td>\n",
       "      <td>30</td>\n",
       "      <td>2019-11-05 11:01:07</td>\n",
       "      <td>2019-11-05 13:17:07</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>19098</td>\n",
       "      <td>[289, 268, 656]</td>\n",
       "      <td>135</td>\n",
       "      <td>2019-09-28 11:15:56</td>\n",
       "      <td>2019-09-28 11:31:22</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22405</th>\n",
       "      <td>19101</td>\n",
       "      <td>[649, 639, 636, 630, 624, 622, 620, 617, 611]</td>\n",
       "      <td>381</td>\n",
       "      <td>2019-09-28 11:15:56</td>\n",
       "      <td>2019-09-28 11:31:22</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22406</th>\n",
       "      <td>19102</td>\n",
       "      <td>[611, 607, 605, 604, 471, 468, 467, 466, 828]</td>\n",
       "      <td>314</td>\n",
       "      <td>2019-09-28 11:15:56</td>\n",
       "      <td>2019-09-28 11:31:22</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22407</th>\n",
       "      <td>19103</td>\n",
       "      <td>[483, 484, 486, 489, 491, 492, 495, 284, 280, ...</td>\n",
       "      <td>225</td>\n",
       "      <td>2019-09-20 18:17:09</td>\n",
       "      <td>2019-09-20 18:24:54</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22408</th>\n",
       "      <td>19104</td>\n",
       "      <td>[169, 566, 546, 521, 733, 1005]</td>\n",
       "      <td>90</td>\n",
       "      <td>2019-11-06 10:05:27</td>\n",
       "      <td>2019-11-06 10:36:28</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22409 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            seg_seq  travel_time  \\\n",
       "0          1  [917, 910, 911, 914, 919, 932, 941, 943, 946, ...          655   \n",
       "1      19105  [640, 631, 627, 613, 603, 598, 594, 592, 898, ...         1753   \n",
       "2      19107  [433, 437, 444, 464, 340, 345, 350, 356, 360, ...          313   \n",
       "3          4           [179, 170, 169, 566, 557, 547, 543, 535]          120   \n",
       "4          5                                 [1010, 1011, 1014]           30   \n",
       "...      ...                                                ...          ...   \n",
       "22404  19098                                    [289, 268, 656]          135   \n",
       "22405  19101      [649, 639, 636, 630, 624, 622, 620, 617, 611]          381   \n",
       "22406  19102      [611, 607, 605, 604, 471, 468, 467, 466, 828]          314   \n",
       "22407  19103  [483, 484, 486, 489, 491, 492, 495, 284, 280, ...          225   \n",
       "22408  19104                    [169, 566, 546, 521, 733, 1005]           90   \n",
       "\n",
       "              start_stamp           end_stamp  dayofweek  start_hour  end_hour  \n",
       "0     2019-09-21 13:18:03 2019-09-21 15:21:49          5          13        15  \n",
       "1     2019-11-23 00:14:00 2019-11-23 02:20:02          5           0         2  \n",
       "2     2019-11-23 00:14:00 2019-11-23 02:20:02          5           0         2  \n",
       "3     2019-11-05 11:01:07 2019-11-05 13:17:07          1          11        13  \n",
       "4     2019-11-05 11:01:07 2019-11-05 13:17:07          1          11        13  \n",
       "...                   ...                 ...        ...         ...       ...  \n",
       "22404 2019-09-28 11:15:56 2019-09-28 11:31:22          5          11        11  \n",
       "22405 2019-09-28 11:15:56 2019-09-28 11:31:22          5          11        11  \n",
       "22406 2019-09-28 11:15:56 2019-09-28 11:31:22          5          11        11  \n",
       "22407 2019-09-20 18:17:09 2019-09-20 18:24:54          4          18        18  \n",
       "22408 2019-11-06 10:05:27 2019-11-06 10:36:28          2          10        10  \n",
       "\n",
       "[22409 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract relevant time data from time stamp\n",
    "traj[\"start_stamp\"] = pd.to_datetime(traj[\"start_stamp\"], unit=\"s\")\n",
    "traj[\"end_stamp\"] = pd.to_datetime(traj[\"end_stamp\"], unit=\"s\")\n",
    "\n",
    "traj[\"dayofweek\"] = traj[\"start_stamp\"].dt.dayofweek\n",
    "traj[\"start_hour\"] = traj[\"start_stamp\"].dt.hour\n",
    "traj[\"end_hour\"] = traj[\"end_stamp\"].dt.hour\n",
    "\n",
    "traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22409/22409 [00:11<00:00, 2006.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# precalc adj matrices\n",
    "adj = GTCModel.generate_node_traj_adj(network=network, traj_data=traj, k=1, bidirectional=True, add_self_loops=True)\n",
    "np.savetxt(\"./traj_adj_k_1_bi_temporal_gtc_small_graph.gz\", X=adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1366, 1366)\n",
      "Epoch: 1, avg_loss: 13.52697298743508\n",
      "Epoch: 2, avg_loss: 9.177679755470969\n",
      "Epoch: 3, avg_loss: 7.557187932910341\n",
      "Epoch: 4, avg_loss: 6.633694020184603\n",
      "Epoch: 5, avg_loss: 5.9950261636213815\n",
      "Epoch: 6, avg_loss: 5.50584077835083\n",
      "Epoch: 7, avg_loss: 5.108271827945461\n",
      "Epoch: 8, avg_loss: 4.773991446603428\n",
      "Epoch: 9, avg_loss: 4.486600589270543\n",
      "Epoch: 10, avg_loss: 4.235654025728051\n",
      "Epoch: 11, avg_loss: 4.014046554723061\n",
      "Epoch: 12, avg_loss: 3.8168586293856297\n",
      "Epoch: 13, avg_loss: 3.6402153727057924\n",
      "Epoch: 14, avg_loss: 3.4811373801974503\n",
      "Epoch: 15, avg_loss: 3.337163337794217\n",
      "Epoch: 16, avg_loss: 3.2063911191441794\n",
      "Epoch: 17, avg_loss: 3.087173185883996\n",
      "Epoch: 18, avg_loss: 2.9781667463707198\n",
      "Epoch: 19, avg_loss: 2.8781468235134504\n",
      "Epoch: 20, avg_loss: 2.7861334605650465\n",
      "Epoch: 21, avg_loss: 2.701243633315676\n",
      "Epoch: 22, avg_loss: 2.622736357460337\n",
      "Epoch: 23, avg_loss: 2.549976542768742\n",
      "Epoch: 24, avg_loss: 2.482372404273712\n",
      "Epoch: 25, avg_loss: 2.4194265896623786\n",
      "Epoch: 26, avg_loss: 2.3606765693301086\n",
      "Epoch: 27, avg_loss: 2.3057419762065514\n",
      "Epoch: 28, avg_loss: 2.254278916623685\n",
      "Epoch: 29, avg_loss: 2.2059799468255714\n"
     ]
    }
   ],
   "source": [
    "# Pretrain tsd model \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "adj_t2v = np.loadtxt(\"./traj_adj_k_1_for_temporal_tsd_small_graph.gz\")\n",
    "print(adj_t2v.shape)\n",
    "data = network.generate_road_segment_pyg_dataset(only_edge_index=True)\n",
    "traj2vec = Traj2VecModel(\n",
    "            data,\n",
    "            network,\n",
    "            adj=adj_t2v,\n",
    "            device=device,\n",
    "            emb_dim=128,\n",
    "            walk_length=30,\n",
    "            context_size=5,\n",
    "            walks_per_node=25,\n",
    "            num_neg=10,\n",
    "        )\n",
    "traj2vec.train(epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj2vec.save_model(path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gtc adj\n",
    "adj = np.loadtxt(\"./traj_adj_k_2_bi_temporal_gtc_small_graph.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tsd pre emb\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "data = network.generate_road_segment_pyg_dataset(only_edge_index=True)\n",
    "tsd = Traj2VecModel(data, network, device=device)\n",
    "tsd.load_model(\"models/model_tsd_small.pt\")\n",
    "tsd_emb = tsd.load_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjaceny matrix for tgcn\n",
    "adj_tgcn = nx.adjacency_matrix(network.line_graph).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5952, 1366, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# load train data\n",
    "data = torch.load(\"../../../datasets/trajectories/hanover/temporal/temporal_data_small.pt\")\n",
    "data = torch.swapaxes(data, 0, 1)\n",
    "data = data.numpy()\n",
    "# eventually remove road label for training\n",
    "data = data[:, :, :-1]\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "edge_index = network.generate_road_segment_pyg_dataset(only_edge_index=True).edge_index\n",
    "# initialize model\n",
    "model = TemporalGraphTrainer(data=data, adj=adj, edge_index=edge_index, struc_emb=tsd_emb, device=device, device_ids=[1], batch_size=16, model_type=ModelVariant.TGTC_BASE, log_name=\"tgtc-noroad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e059708a679b4975ada782b4772a1d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0006309573444801936\n",
      "Restoring states from the checkpoint path at /dstore/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/.lr_find_b45b78f4-eb5e-4d97-9d99-fcc2561bee1e.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006309573444801936"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.find_best_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name     | Type                 | Params\n",
      "--------------------------------------------------\n",
      "0 | encoder  | TemporalGraphEncoder | 67.6 K\n",
      "1 | decoder  | TemporalGraphDecoder | 140 K \n",
      "2 | loss_seq | MSELoss              | 0     \n",
      "--------------------------------------------------\n",
      "208 K     Trainable params\n",
      "0         Non-trainable params\n",
      "208 K     Total params\n",
      "0.832     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4dcfbc8c36448a8340310a04f58932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d3a6416a9c476b950a574540c2da36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b44e66db23d46be843c50c686634b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad14a7fcbf0c466b96bfa3ccb9f3b815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.199 >= min_delta = 0.005. New best score: 2.009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0383c9c4b9546be8a811004fd7035e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.203 >= min_delta = 0.005. New best score: 1.805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fd837f67ef423bab3de6a9cedc5460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.176 >= min_delta = 0.005. New best score: 1.629\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bf11be2a7342469170649eddd55970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.073 >= min_delta = 0.005. New best score: 1.556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e69e0e1fc744ebcae88aeec59bf9fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.051 >= min_delta = 0.005. New best score: 1.505\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619e3282d4de4ff3bc973c84add960dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.043 >= min_delta = 0.005. New best score: 1.462\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d97d9f56be4b8dae5b1561e96d777a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.005. New best score: 1.445\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ec07201a244552a69bb47784a266a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.034 >= min_delta = 0.005. New best score: 1.412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2a3112d4fc4280b839edc1dc464845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.039 >= min_delta = 0.005. New best score: 1.373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f714d2549d4926834dd74e7b15ae35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.005. New best score: 1.356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf6e6a646c34a85848f8426710eaa39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.005. New best score: 1.339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c695d42c21714f5ca029d31c33fc550b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.016 >= min_delta = 0.005. New best score: 1.324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5597c15161bb4e6a8605415a068fdfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.006 >= min_delta = 0.005. New best score: 1.317\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0bd39dc1a1409d91e99b8e5d7ba611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.024 >= min_delta = 0.005. New best score: 1.294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c82870c6c54d30a4bf5f3dca986df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.005. New best score: 1.285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33d381f0d4d4d958b5e30290faf410e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.011 >= min_delta = 0.005. New best score: 1.275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0293c0011e4e5b9b480009b573156b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a12b0715a2b48abb4b6f99808f47937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.029 >= min_delta = 0.005. New best score: 1.246\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23772c49f9c6465092c40c86175c8146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4951d1fc037d4e4c8265b6979154e7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.005. New best score: 1.232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25904a95ccc14c00a2f866cf4885eda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d374c3c02e04caeac3ec98bfe5b9315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e875f6a786fc43e3864a0e03c6a8c9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.027 >= min_delta = 0.005. New best score: 1.205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24091b344cd5421291a146d5bdd3f44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc5134d6f844ac0bfa9105373127640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.005. New best score: 1.195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d83310bb274f048fbbcdea9ece94c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257df7d0f66c41a8a9395cb043925772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.005. New best score: 1.181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ac3ba80b124ef2815c6a7823d583ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17427a3d81947afbdf74cb7d50e4d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.005. New best score: 1.175\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7393b76af14e5f9d135597b3af48a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d26f0442df4f53919a735824b25dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.005. New best score: 1.160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db94e22c93e42f98d41e6f2a74015ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842d8ee1f5cb4c1e951b2bb6e5ae64cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ad9a67cc824ed1a6e93697339f0dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.005. New best score: 1.152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910dcb49b620479aa2d5591a25bbd152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.011 >= min_delta = 0.005. New best score: 1.141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da986d3f304e4efa9aa38198e3653043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e638086a50c406ebbb0846865d07af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad0e47d2acb4eab9d00c3fff9398c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 1.141. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_small_current_best_normal_gnorm.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpascal03/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_model(\u001b[39m\"\u001b[39;49m\u001b[39mmodel_small_current_best_normal_gnorm.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/dstore/home/pheinemeyer/Road-Network-Embedding-Generator/models/tgtc.py:162\u001b[0m, in \u001b[0;36mTemporalGraphTrainer.load_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path, map_location\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    232\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_small_current_best_normal_gnorm.pt'"
     ]
    }
   ],
   "source": [
    "model.load_model(\"model_small_current_best_normal_gnorm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------+\n",
      "|               Modules                | Parameters |\n",
      "+--------------------------------------+------------+\n",
      "| encoder._encoder.graph_conv1.weights |   50688    |\n",
      "| encoder._encoder.graph_conv1.biases  |    384     |\n",
      "| encoder._encoder.graph_conv2.weights |   16896    |\n",
      "| encoder._encoder.graph_conv2.biases  |    128     |\n",
      "|    decoder._tdecoder.weight_ih_l0    |   65536    |\n",
      "|    decoder._tdecoder.weight_hh_l0    |   65536    |\n",
      "|     decoder._tdecoder.bias_ih_l0     |    512     |\n",
      "|     decoder._tdecoder.bias_hh_l0     |    512     |\n",
      "|        decoder.dense.0.weight        |    8192    |\n",
      "|         decoder.dense.0.bias         |     64     |\n",
      "|        decoder.dense.2.weight        |     64     |\n",
      "|         decoder.dense.2.bias         |     1      |\n",
      "+--------------------------------------+------------+\n",
      "Total Trainable Params: 208513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "208513"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model.train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model.state_dict(), os.path.join(\"tgtc-noroad.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1366, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test sequence encoding \n",
    "model.model.to(device).eval()\n",
    "z, _ = model.model(torch.Tensor(data[100:112]).unsqueeze(0).to(device))\n",
    "z = z.squeeze()\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29838128573297384\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# n2v = models[-1]\n",
    "idxs = np.arange(len(network.line_graph.nodes))\n",
    "train_idx, test_idx = model_selection.train_test_split(idxs, test_size=0.2, random_state=69)\n",
    "y = np.array([network.gdf_edges.loc[n][\"highway_enc\"] for n in network.line_graph.nodes])\n",
    "\n",
    "# for m, e in models:\n",
    "    # m.train(epochs=e)\n",
    "    # zn = m.load_emb()\n",
    "    # zcn = np.concatenate((zn, z2), axis=1)\n",
    "    # zct = np.concatenate((zn, z3), axis=1)\n",
    "    # zcnn = np.concatenate((zn, z4), axis=1)\n",
    "    # zctn = np.concatenate((zn, z5), axis=1)\n",
    "    # X = z # embedding for each node\n",
    "eva = [z.detach().cpu().numpy()] # gtc.load_emb(), gae_emb, rand_emb\n",
    "for X in eva:\n",
    "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    lm = linear_model.LogisticRegression(multi_class=\"multinomial\", max_iter=1000)\n",
    "    scorer = make_scorer(metrics.f1_score, average=\"macro\")\n",
    "    print(np.mean(cross_val_score(estimator=lm, X=X, y=y, scoring=scorer, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on traveltime task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['dayofweek', 'start_hour', 'end_hour'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m data, torch\u001b[39m.\u001b[39mTensor(label), torch\u001b[39m.\u001b[39mtensor(time, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m), lengths, mask, \u001b[39mmap\u001b[39m[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m train, test \u001b[39m=\u001b[39m model_selection\u001b[39m.\u001b[39mtrain_test_split(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m             traj, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m69\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     TemporalDataset(train, network),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39mTemporalDataset\u001b[39m.\u001b[39mcollate_fn_padd,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m eval_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     TemporalDataset(test, network),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39mTemporalDataset\u001b[39m.\u001b[39mcollate_fn_padd,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m )\n",
      "\u001b[1;32m/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb Cell 25\u001b[0m in \u001b[0;36mTemporalDataset.__init__\u001b[0;34m(self, data, network)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mseg_seq\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mtravel_time\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime \u001b[39m=\u001b[39m data[[\u001b[39m\"\u001b[39;49m\u001b[39mdayofweek\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstart_hour\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mend_hour\u001b[39;49m\u001b[39m\"\u001b[39;49m]]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork \u001b[39m=\u001b[39m network\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_edge_emb_mapping()\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/pandas/core/indexes/base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5841\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['dayofweek', 'start_hour', 'end_hour'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "class TemporalDataset(Dataset):\n",
    "    def __init__(self, data, network):\n",
    "        self.X = data[\"seg_seq\"].values\n",
    "        self.y = data[\"travel_time\"].values\n",
    "        self.time = data[[\"dayofweek\", \"start_hour\", \"end_hour\"]].values\n",
    "        self.network = network\n",
    "        self.map = self.create_edge_emb_mapping()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=int), self.y[idx], self.time[idx], self.map\n",
    "\n",
    "    # tested index mapping is correct\n",
    "    def create_edge_emb_mapping(self):\n",
    "        # map from trajectory edge id to embedding id\n",
    "        # edge_ids = np.array(self.network.gdf_edges.index, dtype=\"i,i,i\")\n",
    "        # traj_edge_idx = np.array(self.network.gdf_edges.fid)\n",
    "        # node_ids = np.array(self.network.line_graph.nodes, dtype=\"i,i,i\")\n",
    "        # sort_idx = node_ids.argsort()\n",
    "        # emb_ids = sort_idx[np.searchsorted(node_ids, edge_ids, sorter=sort_idx)]\n",
    "        # map = dict(zip(traj_edge_idx, emb_ids))\n",
    "\n",
    "        map = {}\n",
    "        nodes = list(self.network.line_graph.nodes)\n",
    "        for index, id in zip(self.network.gdf_edges.index, self.network.gdf_edges.fid):\n",
    "            map[id] = nodes.index(index)\n",
    "        # print(map == map2) # yields true\n",
    "\n",
    "        return map\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn_padd(batch):\n",
    "        \"\"\"\n",
    "        Padds batch of variable length\n",
    "        \"\"\"\n",
    "        data, label, time, map = zip(*batch)\n",
    "        # seq length for each input in batch\n",
    "        lengths_old = torch.tensor([t.shape[0] for t in data])\n",
    "\n",
    "        # sort data for pad packet, since biggest sequence should be first and then descending order\n",
    "        sort_idxs = torch.argsort(lengths_old, descending=True, dim=0)\n",
    "        lengths = lengths_old[sort_idxs]\n",
    "        data = [\n",
    "            x\n",
    "            for _, x in sorted(\n",
    "                zip(lengths_old.tolist(), data), key=lambda pair: pair[0], reverse=True\n",
    "            )\n",
    "        ]\n",
    "        label = [\n",
    "            x\n",
    "            for _, x in sorted(\n",
    "                zip(lengths_old.tolist(), label), key=lambda pair: pair[0], reverse=True\n",
    "            )\n",
    "        ]\n",
    "        time = [\n",
    "            x\n",
    "            for _, x in sorted(\n",
    "                zip(lengths_old.tolist(), time), key=lambda pair: pair[0], reverse=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # pad\n",
    "        data = torch.nn.utils.rnn.pad_sequence(data, padding_value=0, batch_first=True)\n",
    "        # compute mask\n",
    "        mask = data != 0\n",
    "\n",
    "        return data, torch.Tensor(label), torch.tensor(time, dtype=int), lengths, mask, map[0]\n",
    "\n",
    "\n",
    "train, test = model_selection.train_test_split(\n",
    "            traj, test_size=0.2, random_state=69\n",
    "        )\n",
    "train_loader = DataLoader(\n",
    "    TemporalDataset(train, network),\n",
    "    collate_fn=TemporalDataset.collate_fn_padd,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    TemporalDataset(test, network),\n",
    "    collate_fn=TemporalDataset.collate_fn_padd,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Init \n",
    "class TemporalTest(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        emb_dim: int = 128,\n",
    "        hidden_units: int = 128,\n",
    "        layers: int = 2,\n",
    "        batch_size: int = 128,\n",
    "        plugin=None,\n",
    "    ):\n",
    "        super(TemporalTest, self).__init__()\n",
    "        self.encoder = nn.LSTM(\n",
    "            emb_dim, hidden_units, num_layers=layers, batch_first=True, dropout=0.5\n",
    "        )\n",
    "    \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_units, hidden_units * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units * 2, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, 1),\n",
    "        )\n",
    "        self.hidden_units = hidden_units\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        self.plugin = plugin\n",
    "\n",
    "        self.encoder.to(device)\n",
    "        self.decoder.to(device)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size=batch_size)\n",
    "\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "\n",
    "        x, _ = self.encoder(x)\n",
    "\n",
    "        x, plengths = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            x, batch_first=True, padding_value=0\n",
    "        )\n",
    "        x = x.contiguous()  # batch x seq x hidden\n",
    "        # x = x.view(-1, x.shape[2])\n",
    "        x = torch.stack(\n",
    "            [x[b, plengths[b] - 1] for b in range(batch_size)]\n",
    "        )  # get last valid item per batch batch x hidden\n",
    "\n",
    "\n",
    "        yh = self.decoder(x)\n",
    "\n",
    "        return yh  # (batch x 1)\n",
    "\n",
    "    def train_model(self, loader, epochs=100):\n",
    "        self.train()\n",
    "        for e in range(epochs):\n",
    "            total_loss = 0\n",
    "            for X, y, time, lengths, mask, map in loader:\n",
    "\n",
    "                emb, _ = self.plugin.generate_emb(time)\n",
    "    \n",
    "                emb_batch = self.get_embedding(emb.detach().cpu(), X.clone(), mask, map)\n",
    "                emb_batch = emb_batch.to(self.device)\n",
    "\n",
    "                y = y.to(self.device)\n",
    "                yh = self.forward(emb_batch, lengths)\n",
    "                loss = self.loss(yh.squeeze(), y)\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "\n",
    "            print(f\"Average training loss in episode {e}: {total_loss/len(loader)}\")\n",
    "\n",
    "    def predict(self, loader):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            yhs, ys = [], []\n",
    "            for X, y, time, lengths, mask, map in loader:\n",
    "                emb, _ = self.plugin.generate_emb(time)\n",
    "\n",
    "                emb_batch = self.get_embedding(emb.detach().cpu(), X.clone(), mask, map)\n",
    "                emb_batch = emb_batch.to(self.device)\n",
    "\n",
    "                y = y.to(self.device)\n",
    "                yh = self.forward(emb_batch, lengths)\n",
    "                yhs.extend(yh.tolist())\n",
    "                ys.extend(y.tolist())\n",
    "\n",
    "            return np.array(yhs), np.array(ys)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_a = torch.randn(self.layers, batch_size, self.hidden_units)\n",
    "        hidden_b = torch.randn(self.layers, batch_size, self.hidden_units)\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        hidden_a = hidden_a.to(self.device)\n",
    "        hidden_b = hidden_b.to(self.device)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def get_embedding(self, emb, batch, mask, map):\n",
    "        \"\"\"\n",
    "        Transform batch_size, seq_length, 1 to batch_size, seq_length, emb_size\n",
    "        \"\"\"\n",
    "        res = torch.zeros((batch.shape[0], batch.shape[1], emb.shape[-1]))\n",
    "        for i, seq in enumerate(batch):\n",
    "            idx = i if emb.shape[0] > 1 else 0\n",
    "            emb_ids = itemgetter(*seq[mask[i]].tolist())(map)\n",
    "            res[i, mask[i], :] = emb[idx, emb_ids, :]\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load temporal sensor data \n",
    "temporal = pd.read_csv(\"../../../datasets/trajectories/hanover/temporal/hannover_traffic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize temporal plugin \n",
    "from evaluation.tasks import TemporalEmbeddingPlugin\n",
    "\n",
    "plugin = TemporalEmbeddingPlugin(model.model, network, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin.load_data(\"plugin_data_small.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 1366, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plugin.processed_temp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed \n",
    "torch.save(plugin.processed_temp_data, 'plugin_data_small.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22409it [02:43, 136.77it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, t in tqdm(traj.iterrows()):\n",
    "    z, _ = plugin.generate_emb(t.values[-3:][np.newaxis, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1366, 128])\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = TemporalTest(\n",
    "            plugin=plugin,\n",
    "            device=device,\n",
    "            batch_size=32,\n",
    "            emb_dim=128,\n",
    "            hidden_units=128\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 0: 87829.1031143814\n",
      "Average training loss in episode 1: 22002.679073884527\n",
      "Average training loss in episode 2: 19545.130449862827\n",
      "Average training loss in episode 3: 18723.289141704267\n",
      "Average training loss in episode 4: 18429.44821215951\n",
      "Average training loss in episode 5: 18138.19066773549\n",
      "Average training loss in episode 6: 18028.32034339837\n",
      "Average training loss in episode 7: 17895.940817440256\n",
      "Average training loss in episode 8: 17518.16926865739\n",
      "Average training loss in episode 9: 17453.806573606005\n"
     ]
    }
   ],
   "source": [
    "task.train_model(loader=train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.77319039153713 129.8692814147533\n"
     ]
    }
   ],
   "source": [
    "yh, y = task.predict(loader=eval_loader)\n",
    "print(metrics.mean_absolute_error(yh, y), metrics.mean_squared_error(yh, y, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm -> 80.862\n",
    "lstm_tsd -> 78.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.69705600006567 143.56656361147427\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on gcn/node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GAEModel, GCNEncoder, GATEncoder, Node2VecModel, GTCModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = network.generate_road_segment_pyg_dataset(traj_data=None, include_coords=False, dataset=\"hannover_small\", drop_labels=[\"highway_enc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5374, 0.0218],\n",
       "        [0.5374, 0.0206],\n",
       "        [0.5374, 0.0176],\n",
       "        ...,\n",
       "        [0.3835, 0.0251],\n",
       "        [0.3835, 0.0635],\n",
       "        [0.3835, 0.0534]], dtype=torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, avg_loss: 1.1670883684158324\n",
      "Epoch: 1000, avg_loss: 1.1163232628107072\n",
      "Epoch: 1500, avg_loss: 1.0926555108229319\n",
      "Epoch: 2000, avg_loss: 1.0770727790594101\n",
      "Epoch: 2500, avg_loss: 1.0661062982320786\n",
      "Epoch: 3000, avg_loss: 1.0572415694793065\n",
      "Epoch: 3500, avg_loss: 1.0497092295033592\n",
      "Epoch: 4000, avg_loss: 1.0433547353297472\n",
      "Epoch: 4500, avg_loss: 1.0378585093153847\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# create pyg dataset\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    # T.OneHotDegree(128), # training without features\n",
    "    T.ToDevice(device),\n",
    "])\n",
    "data = transform(data)\n",
    "gae = GAEModel(data, device=device, encoder=GATEncoder, emb_dim=128)\n",
    "gae.train(epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae.save_model(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_z = gae.model.encode(data.x, data.edge_index).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_gtc = np.loadtxt(\"./traj_adj_k_1_bi_temporal_gtc_small_graph.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000, avg_loss: 1.2727549135684968\n",
      "Epoch: 2000, avg_loss: 1.2658498816490173\n",
      "Epoch: 3000, avg_loss: 1.263398430665334\n",
      "Epoch: 4000, avg_loss: 1.2621311684846879\n"
     ]
    }
   ],
   "source": [
    "# train gtc model\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    # T.OneHotDegree(128), # training without features\n",
    "    T.ToDevice(device),\n",
    "])\n",
    "data = transform(data)\n",
    "gtc = GTCModel(data, device, network, adj=adj_gtc)\n",
    "gtc.train(epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtc.save_model(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtc_z = gtc.load_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    T.ToDevice(device),\n",
    "])\n",
    "data = transform(data)\n",
    "n2v = Node2VecModel(data, device=device, q=1, p=1, negative_samples=3)\n",
    "n2v.train(epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v.save_model(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_z = n2v.load_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2864780772047495\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# n2v = models[-1]\n",
    "idxs = np.arange(len(network.line_graph.nodes))\n",
    "train_idx, test_idx = model_selection.train_test_split(idxs, test_size=0.2, random_state=69)\n",
    "y = np.array([network.gdf_edges.loc[n][\"highway_enc\"] for n in network.line_graph.nodes])\n",
    "\n",
    "# for m, e in models:\n",
    "    # m.train(epochs=e)\n",
    "    # zn = m.load_emb()\n",
    "    # zcn = np.concatenate((zn, z2), axis=1)\n",
    "    # zct = np.concatenate((zn, z3), axis=1)\n",
    "    # zcnn = np.concatenate((zn, z4), axis=1)\n",
    "    # zctn = np.concatenate((zn, z5), axis=1)\n",
    "    # X = z # embedding for each node\n",
    "eva = [gtc_z] # gtc.load_emb(), gae_emb, rand_emb\n",
    "for X in eva:\n",
    "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    lm = linear_model.LogisticRegression(multi_class=\"multinomial\", max_iter=1000)\n",
    "    scorer = make_scorer(metrics.f1_score, average=\"macro\")\n",
    "    print(np.mean(cross_val_score(estimator=lm, X=X, y=y, scoring=scorer, cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalPlugin:\n",
    "    def __init__(self, model: nn.Module, network, device):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.network = network\n",
    "\n",
    "    def generate_emb(self, times):\n",
    "        # Bx3 - 3 time features B -> Batch Size\n",
    "        return torch.Tensor(self.model.load_emb()).unsqueeze(0), None\n",
    "\n",
    "plugin = NormalPlugin(gtc, network, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1366, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb, _ = plugin.generate_emb(_)\n",
    "\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = TemporalTest(\n",
    "            plugin=plugin,\n",
    "            device=device,\n",
    "            batch_size=64,\n",
    "            hidden_units=256,\n",
    "            emb_dim=256\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb Cell 56\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpascal02/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m task\u001b[39m.\u001b[39mtrain_model(loader\u001b[39m=\u001b[39mtrain_loader, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "task.train_model(loader=train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.79005843174878 134.66384316477473\n"
     ]
    }
   ],
   "source": [
    "yh, y = task.predict(loader=eval_loader)\n",
    "print(metrics.mean_absolute_error(yh, y), metrics.mean_squared_error(yh, y, squared=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('road')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "088070de2c6b4023b2f7ae556c412f86bcd02589c7bdb3766a0caf3cf4813fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
