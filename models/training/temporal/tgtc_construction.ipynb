{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from generator import RoadNetwork, Trajectory\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from models import TemporalGraphTrainer, GTCModel, Traj2VecModel, ModelVariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load required data\n",
    "unmapped_traj = pd.read_csv(\"../../../datasets/trajectories/hanover/temporal/mapped_id_poly_clipped_small_graph.csv\", \";\")\n",
    "traj = Trajectory(\"../../../datasets/trajectories/hanover/temporal/road_segment_map_final_small_graph.csv\", nrows=100000000).generate_TTE_datatset()\n",
    "traj[\"seg_seq\"] = traj[\"seg_seq\"].map(np.array)\n",
    "traj = traj.join(unmapped_traj[[\"start_stamp\", \"end_stamp\", \"id\"]].set_index(\"id\"), on=\"id\", how=\"left\")\n",
    "network = RoadNetwork()\n",
    "network.load_edges(\"../../../osm_data/hanover_temp_small_graph/\")\n",
    "network.gdf_edges.rename(columns={\"speed_limi\": \"speed_limit\", \"highway_en\": \"highway_enc\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folium Heat Map\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "from collections import defaultdict\n",
    "import branca.colormap\n",
    "\n",
    "m = folium.Map(location=[52.37052, 9.73322],\n",
    "                    zoom_start = 8)\n",
    "\n",
    "sub_df[\"coords\"] = sub_df[\"geometry\"].swifter.apply(lambda x: list(x.coords))\n",
    "coords = sub_df.loc[:, \"coords\"].values\n",
    "\n",
    "for line in coords:\n",
    "    data = [(c[1], c[0]) for c in line]\n",
    "    folium.PolyLine(data, color=\"red\", weight=2.5, opacity=0.8).add_to(m)\n",
    "\n",
    "# coords = gdf[\"coords\"].values\n",
    "# for line in coords:\n",
    "#     data = [(c[1], c[0]) for c in line]\n",
    "#     folium.PolyLine(data, color=\"green\", weight=2.5, opacity=0.8).add_to(m)\n",
    "\n",
    "# Display the map\n",
    "#map_porto.save(\"heatmap_gps_points_porto.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seg_seq</th>\n",
       "      <th>travel_time</th>\n",
       "      <th>start_stamp</th>\n",
       "      <th>end_stamp</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>start_hour</th>\n",
       "      <th>end_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[917, 910, 911, 914, 919, 932, 941, 943, 946, ...</td>\n",
       "      <td>655</td>\n",
       "      <td>2019-09-21 13:18:03</td>\n",
       "      <td>2019-09-21 15:21:49</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19105</td>\n",
       "      <td>[640, 631, 627, 613, 603, 598, 594, 592, 898, ...</td>\n",
       "      <td>1753</td>\n",
       "      <td>2019-11-23 00:14:00</td>\n",
       "      <td>2019-11-23 02:20:02</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19107</td>\n",
       "      <td>[433, 437, 444, 464, 340, 345, 350, 356, 360, ...</td>\n",
       "      <td>313</td>\n",
       "      <td>2019-11-23 00:14:00</td>\n",
       "      <td>2019-11-23 02:20:02</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[179, 170, 169, 566, 557, 547, 543, 535]</td>\n",
       "      <td>120</td>\n",
       "      <td>2019-11-05 11:01:07</td>\n",
       "      <td>2019-11-05 13:17:07</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[1010, 1011, 1014]</td>\n",
       "      <td>30</td>\n",
       "      <td>2019-11-05 11:01:07</td>\n",
       "      <td>2019-11-05 13:17:07</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>19098</td>\n",
       "      <td>[289, 268, 656]</td>\n",
       "      <td>135</td>\n",
       "      <td>2019-09-28 11:15:56</td>\n",
       "      <td>2019-09-28 11:31:22</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22405</th>\n",
       "      <td>19101</td>\n",
       "      <td>[649, 639, 636, 630, 624, 622, 620, 617, 611]</td>\n",
       "      <td>381</td>\n",
       "      <td>2019-09-28 11:15:56</td>\n",
       "      <td>2019-09-28 11:31:22</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22406</th>\n",
       "      <td>19102</td>\n",
       "      <td>[611, 607, 605, 604, 471, 468, 467, 466, 828]</td>\n",
       "      <td>314</td>\n",
       "      <td>2019-09-28 11:15:56</td>\n",
       "      <td>2019-09-28 11:31:22</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22407</th>\n",
       "      <td>19103</td>\n",
       "      <td>[483, 484, 486, 489, 491, 492, 495, 284, 280, ...</td>\n",
       "      <td>225</td>\n",
       "      <td>2019-09-20 18:17:09</td>\n",
       "      <td>2019-09-20 18:24:54</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22408</th>\n",
       "      <td>19104</td>\n",
       "      <td>[169, 566, 546, 521, 733, 1005]</td>\n",
       "      <td>90</td>\n",
       "      <td>2019-11-06 10:05:27</td>\n",
       "      <td>2019-11-06 10:36:28</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22409 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            seg_seq  travel_time  \\\n",
       "0          1  [917, 910, 911, 914, 919, 932, 941, 943, 946, ...          655   \n",
       "1      19105  [640, 631, 627, 613, 603, 598, 594, 592, 898, ...         1753   \n",
       "2      19107  [433, 437, 444, 464, 340, 345, 350, 356, 360, ...          313   \n",
       "3          4           [179, 170, 169, 566, 557, 547, 543, 535]          120   \n",
       "4          5                                 [1010, 1011, 1014]           30   \n",
       "...      ...                                                ...          ...   \n",
       "22404  19098                                    [289, 268, 656]          135   \n",
       "22405  19101      [649, 639, 636, 630, 624, 622, 620, 617, 611]          381   \n",
       "22406  19102      [611, 607, 605, 604, 471, 468, 467, 466, 828]          314   \n",
       "22407  19103  [483, 484, 486, 489, 491, 492, 495, 284, 280, ...          225   \n",
       "22408  19104                    [169, 566, 546, 521, 733, 1005]           90   \n",
       "\n",
       "              start_stamp           end_stamp  dayofweek  start_hour  end_hour  \n",
       "0     2019-09-21 13:18:03 2019-09-21 15:21:49          5          13        15  \n",
       "1     2019-11-23 00:14:00 2019-11-23 02:20:02          5           0         2  \n",
       "2     2019-11-23 00:14:00 2019-11-23 02:20:02          5           0         2  \n",
       "3     2019-11-05 11:01:07 2019-11-05 13:17:07          1          11        13  \n",
       "4     2019-11-05 11:01:07 2019-11-05 13:17:07          1          11        13  \n",
       "...                   ...                 ...        ...         ...       ...  \n",
       "22404 2019-09-28 11:15:56 2019-09-28 11:31:22          5          11        11  \n",
       "22405 2019-09-28 11:15:56 2019-09-28 11:31:22          5          11        11  \n",
       "22406 2019-09-28 11:15:56 2019-09-28 11:31:22          5          11        11  \n",
       "22407 2019-09-20 18:17:09 2019-09-20 18:24:54          4          18        18  \n",
       "22408 2019-11-06 10:05:27 2019-11-06 10:36:28          2          10        10  \n",
       "\n",
       "[22409 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract relevant time data from time stamp\n",
    "traj[\"start_stamp\"] = pd.to_datetime(traj[\"start_stamp\"], unit=\"s\")\n",
    "traj[\"end_stamp\"] = pd.to_datetime(traj[\"end_stamp\"], unit=\"s\")\n",
    "\n",
    "traj[\"dayofweek\"] = traj[\"start_stamp\"].dt.dayofweek\n",
    "traj[\"start_hour\"] = traj[\"start_stamp\"].dt.hour\n",
    "traj[\"end_hour\"] = traj[\"end_stamp\"].dt.hour\n",
    "\n",
    "traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22409/22409 [00:06<00:00, 3451.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# precalc adj matrices\n",
    "adj = GTCModel.generate_node_traj_adj(network=network, traj_data=traj, k=1, bidirectional=False, add_self_loops=False)\n",
    "np.savetxt(\"./traj_adj_k_1_for_temporal_tsd_small_graph.gz\", X=adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1366, 1366)\n",
      "Epoch: 1, avg_loss: 13.52697298743508\n",
      "Epoch: 2, avg_loss: 9.177679755470969\n",
      "Epoch: 3, avg_loss: 7.557187932910341\n",
      "Epoch: 4, avg_loss: 6.633694020184603\n",
      "Epoch: 5, avg_loss: 5.9950261636213815\n",
      "Epoch: 6, avg_loss: 5.50584077835083\n",
      "Epoch: 7, avg_loss: 5.108271827945461\n",
      "Epoch: 8, avg_loss: 4.773991446603428\n",
      "Epoch: 9, avg_loss: 4.486600589270543\n",
      "Epoch: 10, avg_loss: 4.235654025728051\n",
      "Epoch: 11, avg_loss: 4.014046554723061\n",
      "Epoch: 12, avg_loss: 3.8168586293856297\n",
      "Epoch: 13, avg_loss: 3.6402153727057924\n",
      "Epoch: 14, avg_loss: 3.4811373801974503\n",
      "Epoch: 15, avg_loss: 3.337163337794217\n",
      "Epoch: 16, avg_loss: 3.2063911191441794\n",
      "Epoch: 17, avg_loss: 3.087173185883996\n",
      "Epoch: 18, avg_loss: 2.9781667463707198\n",
      "Epoch: 19, avg_loss: 2.8781468235134504\n",
      "Epoch: 20, avg_loss: 2.7861334605650465\n",
      "Epoch: 21, avg_loss: 2.701243633315676\n",
      "Epoch: 22, avg_loss: 2.622736357460337\n",
      "Epoch: 23, avg_loss: 2.549976542768742\n",
      "Epoch: 24, avg_loss: 2.482372404273712\n",
      "Epoch: 25, avg_loss: 2.4194265896623786\n",
      "Epoch: 26, avg_loss: 2.3606765693301086\n",
      "Epoch: 27, avg_loss: 2.3057419762065514\n",
      "Epoch: 28, avg_loss: 2.254278916623685\n",
      "Epoch: 29, avg_loss: 2.2059799468255714\n"
     ]
    }
   ],
   "source": [
    "# Pretrain tsd model \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "adj_t2v = np.loadtxt(\"./traj_adj_k_1_for_temporal_tsd_small_graph.gz\")\n",
    "print(adj_t2v.shape)\n",
    "data = network.generate_road_segment_pyg_dataset(only_edge_index=True)\n",
    "traj2vec = Traj2VecModel(\n",
    "            data,\n",
    "            network,\n",
    "            adj=adj_t2v,\n",
    "            device=device,\n",
    "            emb_dim=128,\n",
    "            walk_length=30,\n",
    "            context_size=5,\n",
    "            walks_per_node=25,\n",
    "            num_neg=10,\n",
    "        )\n",
    "traj2vec.train(epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj2vec.save_model(path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gtc adj\n",
    "adj = np.loadtxt(\"./traj_adj_k_2_bi_temporal_gtc_small_graph.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tsd pre emb\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "data = network.generate_road_segment_pyg_dataset(only_edge_index=True)\n",
    "tsd = Traj2VecModel(data, network, device=device)\n",
    "tsd.load_model(\"models/model_tsd_small.pt\")\n",
    "tsd_emb = tsd.load_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjaceny matrix for tgcn\n",
    "adj_tgcn = nx.adjacency_matrix(network.line_graph).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5952, 1366, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# load train data\n",
    "data = torch.load(\"../../../datasets/trajectories/hanover/temporal/temporal_data_small.pt\")\n",
    "data = torch.swapaxes(data, 0, 1)\n",
    "data = data.numpy()\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "edge_index = network.generate_road_segment_pyg_dataset(only_edge_index=True).edge_index\n",
    "# initialize model\n",
    "model = TemporalGraphTrainer(data=data, adj=adj, edge_index=edge_index, struc_emb=tsd_emb, device=device, device_ids=[3], batch_size=16, model_type=ModelVariant.TGTC_ATT, log_name=\"tgtc-lstm-2layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306525c75b4e4da9a8b88476cf5b3651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=87` reached.\n",
      "LR finder stopped early after 87 steps due to diverging loss.\n",
      "Learning rate set to 0.0005248074602497723\n",
      "Restoring states from the checkpoint path at /dstore/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/.lr_find_ffafb770-80af-40ce-82ac-f7f5f7785f60.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005248074602497723"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.find_best_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: tb_logs/tgtc-lstm-2layer\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name     | Type                     | Params\n",
      "------------------------------------------------------\n",
      "0 | encoder  | TemporalAttentionEncoder | 266 K \n",
      "1 | decoder  | TemporalGraphDecoder     | 140 K \n",
      "2 | loss_seq | MSELoss                  | 0     \n",
      "------------------------------------------------------\n",
      "406 K     Trainable params\n",
      "0         Non-trainable params\n",
      "406 K     Total params\n",
      "1.627     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b842b451de754f75b868ac7cc85e9226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debdb14784654718adc8f50cc1d72604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50d4f0afb0e4797944c57f5d48268f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd5c9eabdb84c75a3a70d20266676f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.159 >= min_delta = 0.005. New best score: 1.898\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_small_current_best_normal_gnorm.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpascal03/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/temporal/tgtc_construction.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_model(\u001b[39m\"\u001b[39;49m\u001b[39mmodel_small_current_best_normal_gnorm.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/dstore/home/pheinemeyer/Road-Network-Embedding-Generator/models/tgtc.py:162\u001b[0m, in \u001b[0;36mTemporalGraphTrainer.load_model\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path, map_location\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    232\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_small_current_best_normal_gnorm.pt'"
     ]
    }
   ],
   "source": [
    "model.load_model(\"model_small_current_best_normal_gnorm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------+\n",
      "|               Modules                | Parameters |\n",
      "+--------------------------------------+------------+\n",
      "| encoder._encoder.graph_conv1.weights |   50688    |\n",
      "| encoder._encoder.graph_conv1.biases  |    384     |\n",
      "| encoder._encoder.graph_conv2.weights |   16896    |\n",
      "| encoder._encoder.graph_conv2.biases  |    128     |\n",
      "|    decoder._tdecoder.weight_ih_l0    |   65536    |\n",
      "|    decoder._tdecoder.weight_hh_l0    |   65536    |\n",
      "|     decoder._tdecoder.bias_ih_l0     |    512     |\n",
      "|     decoder._tdecoder.bias_hh_l0     |    512     |\n",
      "|        decoder.dense.0.weight        |    8192    |\n",
      "|         decoder.dense.0.bias         |     64     |\n",
      "|        decoder.dense.2.weight        |     64     |\n",
      "|         decoder.dense.2.bias         |     1      |\n",
      "+--------------------------------------+------------+\n",
      "Total Trainable Params: 208513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "208513"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model.train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model.state_dict(), os.path.join(\"tgtc-lstm-2layer.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1366, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test sequence encoding \n",
    "model.model.to(device).eval()\n",
    "z, _ = model.model(torch.Tensor(data[100:112]).unsqueeze(0).to(device))\n",
    "z = z.squeeze()\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6549915452310643\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# n2v = models[-1]\n",
    "idxs = np.arange(len(network.line_graph.nodes))\n",
    "train_idx, test_idx = model_selection.train_test_split(idxs, test_size=0.2, random_state=69)\n",
    "y = np.array([network.gdf_edges.loc[n][\"highway_enc\"] for n in network.line_graph.nodes])\n",
    "\n",
    "# for m, e in models:\n",
    "    # m.train(epochs=e)\n",
    "    # zn = m.load_emb()\n",
    "    # zcn = np.concatenate((zn, z2), axis=1)\n",
    "    # zct = np.concatenate((zn, z3), axis=1)\n",
    "    # zcnn = np.concatenate((zn, z4), axis=1)\n",
    "    # zctn = np.concatenate((zn, z5), axis=1)\n",
    "    # X = z # embedding for each node\n",
    "eva = [z.detach().cpu().numpy()] # gtc.load_emb(), gae_emb, rand_emb\n",
    "for X in eva:\n",
    "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    lm = linear_model.LogisticRegression(multi_class=\"multinomial\", max_iter=1000)\n",
    "    scorer = make_scorer(metrics.f1_score, average=\"macro\")\n",
    "    print(np.mean(cross_val_score(estimator=lm, X=X, y=y, scoring=scorer, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on traveltime task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "class TemporalDataset(Dataset):\n",
    "    def __init__(self, data, network):\n",
    "        self.X = data[\"seg_seq\"].values\n",
    "        self.y = data[\"travel_time\"].values\n",
    "        self.time = data[[\"dayofweek\", \"start_hour\", \"end_hour\"]].values\n",
    "        self.network = network\n",
    "        self.map = self.create_edge_emb_mapping()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=int), self.y[idx], self.time[idx], self.map\n",
    "\n",
    "    # tested index mapping is correct\n",
    "    def create_edge_emb_mapping(self):\n",
    "        # map from trajectory edge id to embedding id\n",
    "        # edge_ids = np.array(self.network.gdf_edges.index, dtype=\"i,i,i\")\n",
    "        # traj_edge_idx = np.array(self.network.gdf_edges.fid)\n",
    "        # node_ids = np.array(self.network.line_graph.nodes, dtype=\"i,i,i\")\n",
    "        # sort_idx = node_ids.argsort()\n",
    "        # emb_ids = sort_idx[np.searchsorted(node_ids, edge_ids, sorter=sort_idx)]\n",
    "        # map = dict(zip(traj_edge_idx, emb_ids))\n",
    "\n",
    "        map = {}\n",
    "        nodes = list(self.network.line_graph.nodes)\n",
    "        for index, id in zip(self.network.gdf_edges.index, self.network.gdf_edges.fid):\n",
    "            map[id] = nodes.index(index)\n",
    "        # print(map == map2) # yields true\n",
    "\n",
    "        return map\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn_padd(batch):\n",
    "        \"\"\"\n",
    "        Padds batch of variable length\n",
    "        \"\"\"\n",
    "        data, label, time, map = zip(*batch)\n",
    "        # seq length for each input in batch\n",
    "        lengths_old = torch.tensor([t.shape[0] for t in data])\n",
    "\n",
    "        # sort data for pad packet, since biggest sequence should be first and then descending order\n",
    "        sort_idxs = torch.argsort(lengths_old, descending=True, dim=0)\n",
    "        lengths = lengths_old[sort_idxs]\n",
    "        data = [\n",
    "            x\n",
    "            for _, x in sorted(\n",
    "                zip(lengths_old.tolist(), data), key=lambda pair: pair[0], reverse=True\n",
    "            )\n",
    "        ]\n",
    "        label = [\n",
    "            x\n",
    "            for _, x in sorted(\n",
    "                zip(lengths_old.tolist(), label), key=lambda pair: pair[0], reverse=True\n",
    "            )\n",
    "        ]\n",
    "        time = [\n",
    "            x\n",
    "            for _, x in sorted(\n",
    "                zip(lengths_old.tolist(), time), key=lambda pair: pair[0], reverse=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # pad\n",
    "        data = torch.nn.utils.rnn.pad_sequence(data, padding_value=0, batch_first=True)\n",
    "        # compute mask\n",
    "        mask = data != 0\n",
    "\n",
    "        return data, torch.Tensor(label), torch.tensor(time, dtype=int), lengths, mask, map[0]\n",
    "\n",
    "\n",
    "train, test = model_selection.train_test_split(\n",
    "            traj, test_size=0.2, random_state=69\n",
    "        )\n",
    "train_loader = DataLoader(\n",
    "    TemporalDataset(train, network),\n",
    "    collate_fn=TemporalDataset.collate_fn_padd,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    TemporalDataset(test, network),\n",
    "    collate_fn=TemporalDataset.collate_fn_padd,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Init \n",
    "class TemporalTest(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        emb_dim: int = 128,\n",
    "        hidden_units: int = 128,\n",
    "        layers: int = 2,\n",
    "        batch_size: int = 128,\n",
    "        plugin=None,\n",
    "    ):\n",
    "        super(TemporalTest, self).__init__()\n",
    "        self.encoder = nn.LSTM(\n",
    "            emb_dim, hidden_units, num_layers=layers, batch_first=True, dropout=0.5\n",
    "        )\n",
    "    \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_units, hidden_units * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units * 2, hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units, 1),\n",
    "        )\n",
    "        self.hidden_units = hidden_units\n",
    "        self.layers = layers\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        self.plugin = plugin\n",
    "\n",
    "        self.encoder.to(device)\n",
    "        self.decoder.to(device)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size=batch_size)\n",
    "\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "\n",
    "        x, _ = self.encoder(x)\n",
    "\n",
    "        x, plengths = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            x, batch_first=True, padding_value=0\n",
    "        )\n",
    "        x = x.contiguous()  # batch x seq x hidden\n",
    "        # x = x.view(-1, x.shape[2])\n",
    "        x = torch.stack(\n",
    "            [x[b, plengths[b] - 1] for b in range(batch_size)]\n",
    "        )  # get last valid item per batch batch x hidden\n",
    "\n",
    "\n",
    "        yh = self.decoder(x)\n",
    "\n",
    "        return yh  # (batch x 1)\n",
    "\n",
    "    def train_model(self, loader, epochs=100):\n",
    "        self.train()\n",
    "        for e in range(epochs):\n",
    "            total_loss = 0\n",
    "            for X, y, time, lengths, mask, map in loader:\n",
    "\n",
    "                emb, _ = self.plugin.generate_emb(time)\n",
    "    \n",
    "                emb_batch = self.get_embedding(emb.detach().cpu(), X.clone(), mask, map)\n",
    "                emb_batch = emb_batch.to(self.device)\n",
    "\n",
    "                y = y.to(self.device)\n",
    "                yh = self.forward(emb_batch, lengths)\n",
    "                loss = self.loss(yh.squeeze(), y)\n",
    "\n",
    "                self.opt.zero_grad()\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "\n",
    "            print(f\"Average training loss in episode {e}: {total_loss/len(loader)}\")\n",
    "\n",
    "    def predict(self, loader):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            yhs, ys = [], []\n",
    "            for X, y, time, lengths, mask, map in loader:\n",
    "                emb, _ = self.plugin.generate_emb(time)\n",
    "\n",
    "                emb_batch = self.get_embedding(emb.detach().cpu(), X.clone(), mask, map)\n",
    "                emb_batch = emb_batch.to(self.device)\n",
    "\n",
    "                y = y.to(self.device)\n",
    "                yh = self.forward(emb_batch, lengths)\n",
    "                yhs.extend(yh.tolist())\n",
    "                ys.extend(y.tolist())\n",
    "\n",
    "            return np.array(yhs), np.array(ys)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_a = torch.randn(self.layers, batch_size, self.hidden_units)\n",
    "        hidden_b = torch.randn(self.layers, batch_size, self.hidden_units)\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        hidden_a = hidden_a.to(self.device)\n",
    "        hidden_b = hidden_b.to(self.device)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def get_embedding(self, emb, batch, mask, map):\n",
    "        \"\"\"\n",
    "        Transform batch_size, seq_length, 1 to batch_size, seq_length, emb_size\n",
    "        \"\"\"\n",
    "        res = torch.zeros((batch.shape[0], batch.shape[1], emb.shape[-1]))\n",
    "        for i, seq in enumerate(batch):\n",
    "            idx = i if emb.shape[0] > 1 else 0\n",
    "            emb_ids = itemgetter(*seq[mask[i]].tolist())(map)\n",
    "            res[i, mask[i], :] = emb[idx, emb_ids, :]\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load temporal sensor data \n",
    "temporal = pd.read_csv(\"../../../datasets/trajectories/hanover/temporal/hannover_traffic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize temporal plugin \n",
    "from evaluation.tasks import TemporalEmbeddingPlugin\n",
    "\n",
    "plugin = TemporalEmbeddingPlugin(model.model, network, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin.load_data(\"plugin_data_small.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 1366, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plugin.processed_temp_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed \n",
    "torch.save(plugin.processed_temp_data, 'plugin_data_small.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22409it [02:43, 136.77it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, t in tqdm(traj.iterrows()):\n",
    "    z, _ = plugin.generate_emb(t.values[-3:][np.newaxis, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1366, 128])\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = TemporalTest(\n",
    "            plugin=plugin,\n",
    "            device=device,\n",
    "            batch_size=32,\n",
    "            emb_dim=128,\n",
    "            hidden_units=128\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 0: 87829.1031143814\n",
      "Average training loss in episode 1: 22002.679073884527\n",
      "Average training loss in episode 2: 19545.130449862827\n",
      "Average training loss in episode 3: 18723.289141704267\n",
      "Average training loss in episode 4: 18429.44821215951\n",
      "Average training loss in episode 5: 18138.19066773549\n",
      "Average training loss in episode 6: 18028.32034339837\n",
      "Average training loss in episode 7: 17895.940817440256\n",
      "Average training loss in episode 8: 17518.16926865739\n",
      "Average training loss in episode 9: 17453.806573606005\n"
     ]
    }
   ],
   "source": [
    "task.train_model(loader=train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.77319039153713 129.8692814147533\n"
     ]
    }
   ],
   "source": [
    "yh, y = task.predict(loader=eval_loader)\n",
    "print(metrics.mean_absolute_error(yh, y), metrics.mean_squared_error(yh, y, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm -> 80.862\n",
    "lstm_tsd -> 78.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.69705600006567 143.56656361147427\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on gcn/node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import GAEModel, GCNEncoder, GATEncoder, Node2VecModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = network.generate_road_segment_pyg_dataset(traj_data=None, include_coords=False, dataset=\"hannover_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, avg_loss: 1.1257418394088745\n",
      "Epoch: 1000, avg_loss: 1.0708516818881035\n",
      "Epoch: 1500, avg_loss: 1.0423320540189742\n",
      "Epoch: 2000, avg_loss: 1.0241585468947887\n",
      "Epoch: 2500, avg_loss: 1.0099976078748703\n",
      "Epoch: 3000, avg_loss: 0.9990644570589066\n",
      "Epoch: 3500, avg_loss: 0.990465463416917\n",
      "Epoch: 4000, avg_loss: 0.9834868279695511\n",
      "Epoch: 4500, avg_loss: 0.9776899226374096\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "# create pyg dataset\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    # T.OneHotDegree(128), # training without features\n",
    "    T.ToDevice(device),\n",
    "])\n",
    "data = transform(data)\n",
    "gae = GAEModel(data, device=device, encoder=GATEncoder, emb_dim=128)\n",
    "gae.train(epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae.save_model(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_z = gae.model.encode(data.x, data.edge_index).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "transform = T.Compose([\n",
    "    T.ToDevice(device),\n",
    "])\n",
    "data = transform(data)\n",
    "n2v = Node2VecModel(data, device=device, q=1, p=1, negative_samples=3)\n",
    "n2v.train(epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v.save_model(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_z = n2v.load_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4663178397023019\n",
      "0.21722996450955182\n",
      "0.5198928335690597\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# n2v = models[-1]\n",
    "idxs = np.arange(len(network.line_graph.nodes))\n",
    "train_idx, test_idx = model_selection.train_test_split(idxs, test_size=0.2, random_state=69)\n",
    "y = np.array([network.gdf_edges.loc[n][\"highway_enc\"] for n in network.line_graph.nodes])\n",
    "\n",
    "# for m, e in models:\n",
    "    # m.train(epochs=e)\n",
    "    # zn = m.load_emb()\n",
    "    # zcn = np.concatenate((zn, z2), axis=1)\n",
    "    # zct = np.concatenate((zn, z3), axis=1)\n",
    "    # zcnn = np.concatenate((zn, z4), axis=1)\n",
    "    # zctn = np.concatenate((zn, z5), axis=1)\n",
    "    # X = z # embedding for each node\n",
    "eva = [gae_z, n2v_z, np.concatenate([gae_z, n2v_z], axis=-1)] # gtc.load_emb(), gae_emb, rand_emb\n",
    "for X in eva:\n",
    "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    lm = linear_model.LogisticRegression(multi_class=\"multinomial\", max_iter=1000)\n",
    "    scorer = make_scorer(metrics.f1_score, average=\"macro\")\n",
    "    print(np.mean(cross_val_score(estimator=lm, X=X, y=y, scoring=scorer, cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalPlugin:\n",
    "    def __init__(self, model: nn.Module, network, device):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.network = network\n",
    "\n",
    "    def generate_emb(self, times):\n",
    "        # Bx3 - 3 time features B -> Batch Size\n",
    "        return torch.Tensor(np.concatenate([self.model[0].load_emb(), self.model[1].load_emb()], axis=-1)).unsqueeze(0), None\n",
    "\n",
    "plugin = NormalPlugin((gae, n2v), network, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1366, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb, _ = plugin.generate_emb(_)\n",
    "\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = TemporalTest(\n",
    "            plugin=plugin,\n",
    "            device=device,\n",
    "            batch_size=64,\n",
    "            hidden_units=256,\n",
    "            emb_dim=256\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 0: 58252.96916560481\n",
      "Average training loss in episode 1: 20444.590375550077\n",
      "Average training loss in episode 2: 19621.356012300163\n",
      "Average training loss in episode 3: 19542.15976719168\n",
      "Average training loss in episode 4: 19454.929917279413\n",
      "Average training loss in episode 5: 19105.126787753452\n",
      "Average training loss in episode 6: 18854.38008256392\n",
      "Average training loss in episode 7: 18727.521045705213\n",
      "Average training loss in episode 8: 18596.11075933392\n",
      "Average training loss in episode 9: 18494.71521844711\n",
      "Average training loss in episode 10: 18593.629486791164\n",
      "Average training loss in episode 11: 18346.102795823586\n",
      "Average training loss in episode 12: 18313.28846063461\n",
      "Average training loss in episode 13: 18075.04020835074\n",
      "Average training loss in episode 14: 18244.0101777483\n",
      "Average training loss in episode 15: 17858.406213879374\n",
      "Average training loss in episode 16: 17779.946660713293\n",
      "Average training loss in episode 17: 17683.174168442234\n",
      "Average training loss in episode 18: 17568.23433017561\n",
      "Average training loss in episode 19: 17372.09075416388\n"
     ]
    }
   ],
   "source": [
    "task.train_model(loader=train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.79005843174878 134.66384316477473\n"
     ]
    }
   ],
   "source": [
    "yh, y = task.predict(loader=eval_loader)\n",
    "print(metrics.mean_absolute_error(yh, y), metrics.mean_squared_error(yh, y, squared=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('road')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "088070de2c6b4023b2f7ae556c412f86bcd02589c7bdb3766a0caf3cf4813fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
