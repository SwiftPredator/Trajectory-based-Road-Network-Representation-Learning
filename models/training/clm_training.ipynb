{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from generator import RoadNetwork, Trajectory\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from models import CLMModel\n",
    "from models.utils import generate_trajid_to_nodeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"sf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = RoadNetwork()\n",
    "network.load(f\"../../osm_data/{city}\")\n",
    "trajectory = pd.read_pickle(\n",
    "    f\"../../datasets/trajectories/{city}/traj_train_test_split/train_69.pkl\"\n",
    ")\n",
    "trajectory[\"seg_seq\"] = trajectory[\"seg_seq\"].map(np.array)\n",
    "data = network.generate_road_segment_pyg_dataset(include_coords=True, dataset=city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080963/1080963 [16:02<00:00, 1123.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# calculate transition matrix \n",
    "traj_map = generate_trajid_to_nodeid(network)\n",
    "trans_mat = np.zeros((data.x.shape[0], data.x.shape[0]))\n",
    "for seq in tqdm(trajectory.seg_seq):\n",
    "    for i, id1 in enumerate(seq):\n",
    "        for id2 in seq[i:]:\n",
    "            node_id1, node_id2 = traj_map[id1], traj_map[id2]\n",
    "            trans_mat[node_id1, node_id2] += 1\n",
    "\n",
    "trans_mat = trans_mat / (trans_mat.max(axis=1, keepdims=True, initial=0.) + 1e-9)\n",
    "row, col = np.diag_indices_from(trans_mat)\n",
    "trans_mat[row, col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"clm_trans_mat_porto.npy\", trans_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat = np.load(f\"./clm_trans_mat_{city}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_mat_b = (trans_mat > 0.6)\n",
    "aug_edges = [(i // trans_mat.shape[0] , i % trans_mat.shape[0]) for i, n in enumerate(trans_mat_b.flatten()) if n]\n",
    "aug_edge_index = torch.tensor(np.array(aug_edges).transpose()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trajectory.rename({\"seg_seq\": \"path\"}, inplace=True, axis=1)\n",
    "model = CLMModel(data, device, network, trans_adj=aug_edge_index, traj_data=trajectory, batch_size=64, emb_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+------------+\n",
      "|                                  Modules                                  | Parameters |\n",
      "+---------------------------------------------------------------------------+------------+\n",
      "|                        module.node_embedding.weight                       |  1450368   |\n",
      "|                   module.graph_encoder1.layers.0.att_src                  |    128     |\n",
      "|                   module.graph_encoder1.layers.0.att_dst                  |    128     |\n",
      "|                    module.graph_encoder1.layers.0.bias                    |    128     |\n",
      "|               module.graph_encoder1.layers.0.lin_src.weight               |   16384    |\n",
      "|                   module.graph_encoder1.layers.1.att_src                  |    128     |\n",
      "|                   module.graph_encoder1.layers.1.att_dst                  |    128     |\n",
      "|                    module.graph_encoder1.layers.1.bias                    |    128     |\n",
      "|               module.graph_encoder1.layers.1.lin_src.weight               |   16384    |\n",
      "|                   module.graph_encoder2.layers.0.att_src                  |    128     |\n",
      "|                   module.graph_encoder2.layers.0.att_dst                  |    128     |\n",
      "|                    module.graph_encoder2.layers.0.bias                    |    128     |\n",
      "|               module.graph_encoder2.layers.0.lin_src.weight               |   16384    |\n",
      "|                   module.graph_encoder2.layers.1.att_src                  |    128     |\n",
      "|                   module.graph_encoder2.layers.1.att_dst                  |    128     |\n",
      "|                    module.graph_encoder2.layers.1.bias                    |    128     |\n",
      "|               module.graph_encoder2.layers.1.lin_src.weight               |   16384    |\n",
      "|  module.seq_encoder.transformer_encoder.layers.0.self_attn.in_proj_weight |   49152    |\n",
      "|   module.seq_encoder.transformer_encoder.layers.0.self_attn.in_proj_bias  |    384     |\n",
      "| module.seq_encoder.transformer_encoder.layers.0.self_attn.out_proj.weight |   16384    |\n",
      "|  module.seq_encoder.transformer_encoder.layers.0.self_attn.out_proj.bias  |    128     |\n",
      "|       module.seq_encoder.transformer_encoder.layers.0.linear1.weight      |   16384    |\n",
      "|        module.seq_encoder.transformer_encoder.layers.0.linear1.bias       |    128     |\n",
      "|       module.seq_encoder.transformer_encoder.layers.0.linear2.weight      |   16384    |\n",
      "|        module.seq_encoder.transformer_encoder.layers.0.linear2.bias       |    128     |\n",
      "|        module.seq_encoder.transformer_encoder.layers.0.norm1.weight       |    128     |\n",
      "|         module.seq_encoder.transformer_encoder.layers.0.norm1.bias        |    128     |\n",
      "|        module.seq_encoder.transformer_encoder.layers.0.norm2.weight       |    128     |\n",
      "|         module.seq_encoder.transformer_encoder.layers.0.norm2.bias        |    128     |\n",
      "|  module.seq_encoder.transformer_encoder.layers.1.self_attn.in_proj_weight |   49152    |\n",
      "|   module.seq_encoder.transformer_encoder.layers.1.self_attn.in_proj_bias  |    384     |\n",
      "| module.seq_encoder.transformer_encoder.layers.1.self_attn.out_proj.weight |   16384    |\n",
      "|  module.seq_encoder.transformer_encoder.layers.1.self_attn.out_proj.bias  |    128     |\n",
      "|       module.seq_encoder.transformer_encoder.layers.1.linear1.weight      |   16384    |\n",
      "|        module.seq_encoder.transformer_encoder.layers.1.linear1.bias       |    128     |\n",
      "|       module.seq_encoder.transformer_encoder.layers.1.linear2.weight      |   16384    |\n",
      "|        module.seq_encoder.transformer_encoder.layers.1.linear2.bias       |    128     |\n",
      "|        module.seq_encoder.transformer_encoder.layers.1.norm1.weight       |    128     |\n",
      "|         module.seq_encoder.transformer_encoder.layers.1.norm1.bias        |    128     |\n",
      "|        module.seq_encoder.transformer_encoder.layers.1.norm2.weight       |    128     |\n",
      "|         module.seq_encoder.transformer_encoder.layers.1.norm2.bias        |    128     |\n",
      "+---------------------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 1716608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1716608"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/clm_training.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpascal03/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/clm_training.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bpascal03/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/clm_training.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdel\u001b[39;00m model\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpascal03/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/clm_training.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:47<00:00, 5218.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-04 14:14:33 | (Train) | Epoch=0, batch=200 loss=-0.6331, loss_ss=0.0100,  loss_tt=-0.9375,  loss_st1=-0.6732, loss_st2=-0.6776\n",
      "12-04 14:17:38 | (Train) | Epoch=0, batch=400 loss=-0.7810, loss_ss=-0.0114,  loss_tt=-1.0704,  loss_st1=-0.8403, loss_st2=-0.8417\n",
      "12-04 14:20:47 | (Train) | Epoch=0, batch=600 loss=-0.9322, loss_ss=-0.0335,  loss_tt=-1.1812,  loss_st1=-1.0277, loss_st2=-0.9991\n",
      "12-04 14:24:08 | (Train) | Epoch=0, batch=800 loss=-0.8848, loss_ss=-0.0542,  loss_tt=-1.2448,  loss_st1=-0.9420, loss_st2=-0.9453\n",
      "12-04 14:27:46 | (Train) | Epoch=0, batch=1000 loss=-0.9238, loss_ss=-0.0723,  loss_tt=-1.2390,  loss_st1=-1.0071, loss_st2=-0.9745\n",
      "12-04 14:32:06 | (Train) | Epoch=0, batch=1200 loss=-0.9867, loss_ss=-0.0932,  loss_tt=-1.2338,  loss_st1=-1.0817, loss_st2=-1.0533\n",
      "12-04 14:37:16 | (Train) | Epoch=0, batch=1400 loss=-1.0298, loss_ss=-0.1132,  loss_tt=-1.3352,  loss_st1=-1.1229, loss_st2=-1.0896\n",
      "12-04 14:41:43 | (Train) | Epoch=0, batch=1600 loss=-1.0406, loss_ss=-0.1328,  loss_tt=-1.2942,  loss_st1=-1.1313, loss_st2=-1.1135\n",
      "12-04 14:46:12 | (Train) | Epoch=0, batch=1800 loss=-1.0633, loss_ss=-0.1505,  loss_tt=-1.3313,  loss_st1=-1.1663, loss_st2=-1.1216\n",
      "12-04 14:50:33 | (Train) | Epoch=0, batch=2000 loss=-1.0714, loss_ss=-0.1706,  loss_tt=-1.3061,  loss_st1=-1.1619, loss_st2=-1.1475\n",
      "12-04 14:54:50 | (Train) | Epoch=0, batch=2200 loss=-1.0921, loss_ss=-0.1881,  loss_tt=-1.3246,  loss_st1=-1.1900, loss_st2=-1.1622\n",
      "12-04 14:59:08 | (Train) | Epoch=0, batch=2400 loss=-1.1110, loss_ss=-0.2052,  loss_tt=-1.3518,  loss_st1=-1.2027, loss_st2=-1.1854\n",
      "12-04 15:03:25 | (Train) | Epoch=0, batch=2600 loss=-1.1181, loss_ss=-0.2220,  loss_tt=-1.3074,  loss_st1=-1.2149, loss_st2=-1.1980\n",
      "12-04 15:07:20 | (Train) | Epoch=0, batch=2800 loss=-1.1229, loss_ss=-0.2379,  loss_tt=-1.3455,  loss_st1=-1.2057, loss_st2=-1.2056\n",
      "12-04 15:11:13 | (Train) | Epoch=0, batch=3000 loss=-1.1178, loss_ss=-0.2538,  loss_tt=-1.3163,  loss_st1=-1.2125, loss_st2=-1.1895\n",
      "12-04 15:15:00 | (Train) | Epoch=0, batch=3200 loss=-1.1406, loss_ss=-0.2661,  loss_tt=-1.3341,  loss_st1=-1.2325, loss_st2=-1.2190\n",
      "12-04 15:18:39 | (Train) | Epoch=0, batch=3400 loss=-1.1536, loss_ss=-0.2775,  loss_tt=-1.3452,  loss_st1=-1.2399, loss_st2=-1.2383\n",
      "12-04 15:21:49 | (Train) | Epoch=0, batch=3600 loss=-1.1467, loss_ss=-0.2937,  loss_tt=-1.3235,  loss_st1=-1.2363, loss_st2=-1.2262\n",
      "12-04 15:24:56 | (Train) | Epoch=0, batch=3800 loss=-1.1640, loss_ss=-0.3056,  loss_tt=-1.3666,  loss_st1=-1.2467, loss_st2=-1.2452\n",
      "Epoch: 0, loss: -3953.980489999056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:47<00:00, 5292.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-04 15:30:32 | (Train) | Epoch=1, batch=200 loss=-1.1813, loss_ss=-0.3231,  loss_tt=-1.3382,  loss_st1=-1.2720, loss_st2=-1.2660\n",
      "12-04 15:33:47 | (Train) | Epoch=1, batch=400 loss=-1.1622, loss_ss=-0.3330,  loss_tt=-1.3550,  loss_st1=-1.2470, loss_st2=-1.2366\n",
      "12-04 15:36:55 | (Train) | Epoch=1, batch=600 loss=-1.1728, loss_ss=-0.3430,  loss_tt=-1.3487,  loss_st1=-1.2565, loss_st2=-1.2525\n",
      "12-04 15:40:03 | (Train) | Epoch=1, batch=800 loss=-1.1821, loss_ss=-0.3531,  loss_tt=-1.3605,  loss_st1=-1.2642, loss_st2=-1.2626\n",
      "12-04 15:43:11 | (Train) | Epoch=1, batch=1000 loss=-1.1752, loss_ss=-0.3612,  loss_tt=-1.3520,  loss_st1=-1.2584, loss_st2=-1.2513\n",
      "12-04 15:46:20 | (Train) | Epoch=1, batch=1200 loss=-1.1677, loss_ss=-0.3696,  loss_tt=-1.3545,  loss_st1=-1.2443, loss_st2=-1.2438\n",
      "12-04 15:49:31 | (Train) | Epoch=1, batch=1400 loss=-1.1810, loss_ss=-0.3768,  loss_tt=-1.3618,  loss_st1=-1.2589, loss_st2=-1.2590\n",
      "12-04 15:52:37 | (Train) | Epoch=1, batch=1600 loss=-1.2031, loss_ss=-0.3838,  loss_tt=-1.3528,  loss_st1=-1.2890, loss_st2=-1.2845\n",
      "12-04 15:55:45 | (Train) | Epoch=1, batch=1800 loss=-1.1927, loss_ss=-0.3903,  loss_tt=-1.3652,  loss_st1=-1.2699, loss_st2=-1.2729\n",
      "12-04 15:58:51 | (Train) | Epoch=1, batch=2000 loss=-1.1870, loss_ss=-0.3952,  loss_tt=-1.3591,  loss_st1=-1.2629, loss_st2=-1.2661\n",
      "12-04 16:01:59 | (Train) | Epoch=1, batch=2200 loss=-1.1804, loss_ss=-0.4012,  loss_tt=-1.3720,  loss_st1=-1.2499, loss_st2=-1.2578\n",
      "12-04 16:05:07 | (Train) | Epoch=1, batch=2400 loss=-1.1839, loss_ss=-0.4068,  loss_tt=-1.3612,  loss_st1=-1.2609, loss_st2=-1.2568\n",
      "12-04 16:08:19 | (Train) | Epoch=1, batch=2600 loss=-1.1962, loss_ss=-0.4100,  loss_tt=-1.3681,  loss_st1=-1.2771, loss_st2=-1.2689\n",
      "12-04 16:11:30 | (Train) | Epoch=1, batch=2800 loss=-1.2000, loss_ss=-0.4153,  loss_tt=-1.3738,  loss_st1=-1.2746, loss_st2=-1.2782\n",
      "12-04 16:14:38 | (Train) | Epoch=1, batch=3000 loss=-1.1976, loss_ss=-0.4193,  loss_tt=-1.3634,  loss_st1=-1.2767, loss_st2=-1.2715\n",
      "12-04 16:17:44 | (Train) | Epoch=1, batch=3200 loss=-1.2104, loss_ss=-0.4239,  loss_tt=-1.3770,  loss_st1=-1.2905, loss_st2=-1.2853\n",
      "12-04 16:20:53 | (Train) | Epoch=1, batch=3400 loss=-1.2038, loss_ss=-0.4282,  loss_tt=-1.3714,  loss_st1=-1.2754, loss_st2=-1.2841\n",
      "12-04 16:24:02 | (Train) | Epoch=1, batch=3600 loss=-1.2065, loss_ss=-0.4305,  loss_tt=-1.3484,  loss_st1=-1.2823, loss_st2=-1.2893\n",
      "12-04 16:27:09 | (Train) | Epoch=1, batch=3800 loss=-1.2056, loss_ss=-0.4337,  loss_tt=-1.3595,  loss_st1=-1.2856, loss_st2=-1.2799\n",
      "Epoch: 1, loss: -4633.588268399239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:46<00:00, 5430.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-04 16:32:47 | (Train) | Epoch=2, batch=200 loss=-1.2068, loss_ss=-0.4388,  loss_tt=-1.3628,  loss_st1=-1.2812, loss_st2=-1.2854\n",
      "12-04 16:35:53 | (Train) | Epoch=2, batch=400 loss=-1.2109, loss_ss=-0.4426,  loss_tt=-1.3510,  loss_st1=-1.2904, loss_st2=-1.2885\n",
      "12-04 16:38:59 | (Train) | Epoch=2, batch=600 loss=-1.2096, loss_ss=-0.4449,  loss_tt=-1.3748,  loss_st1=-1.2818, loss_st2=-1.2872\n",
      "12-04 16:42:06 | (Train) | Epoch=2, batch=800 loss=-1.2165, loss_ss=-0.4481,  loss_tt=-1.3567,  loss_st1=-1.2963, loss_st2=-1.2938\n",
      "12-04 16:45:13 | (Train) | Epoch=2, batch=1000 loss=-1.2225, loss_ss=-0.4501,  loss_tt=-1.3707,  loss_st1=-1.2995, loss_st2=-1.3016\n",
      "12-04 16:48:20 | (Train) | Epoch=2, batch=1200 loss=-1.2126, loss_ss=-0.4528,  loss_tt=-1.3603,  loss_st1=-1.2905, loss_st2=-1.2878\n",
      "12-04 16:51:26 | (Train) | Epoch=2, batch=1400 loss=-1.2197, loss_ss=-0.4547,  loss_tt=-1.3616,  loss_st1=-1.2981, loss_st2=-1.2971\n",
      "12-04 16:54:33 | (Train) | Epoch=2, batch=1600 loss=-1.2112, loss_ss=-0.4582,  loss_tt=-1.3594,  loss_st1=-1.2870, loss_st2=-1.2866\n",
      "12-04 16:57:47 | (Train) | Epoch=2, batch=1800 loss=-1.2100, loss_ss=-0.4605,  loss_tt=-1.2754,  loss_st1=-1.2872, loss_st2=-1.3038\n",
      "12-04 17:00:54 | (Train) | Epoch=2, batch=2000 loss=-1.2340, loss_ss=-0.4628,  loss_tt=-1.3718,  loss_st1=-1.3110, loss_st2=-1.3155\n",
      "12-04 17:04:07 | (Train) | Epoch=2, batch=2200 loss=-1.2247, loss_ss=-0.4650,  loss_tt=-1.3533,  loss_st1=-1.3024, loss_st2=-1.3047\n",
      "12-04 17:07:14 | (Train) | Epoch=2, batch=2400 loss=-1.2127, loss_ss=-0.4667,  loss_tt=-1.3717,  loss_st1=-1.2878, loss_st2=-1.2844\n",
      "12-04 17:10:23 | (Train) | Epoch=2, batch=2600 loss=-1.2253, loss_ss=-0.4678,  loss_tt=-1.3728,  loss_st1=-1.3025, loss_st2=-1.3006\n",
      "12-04 17:13:33 | (Train) | Epoch=2, batch=2800 loss=-1.2200, loss_ss=-0.4701,  loss_tt=-1.3592,  loss_st1=-1.2985, loss_st2=-1.2943\n",
      "12-04 17:16:38 | (Train) | Epoch=2, batch=3000 loss=-1.2140, loss_ss=-0.4710,  loss_tt=-1.3783,  loss_st1=-1.2865, loss_st2=-1.2861\n",
      "12-04 17:19:45 | (Train) | Epoch=2, batch=3200 loss=-1.2237, loss_ss=-0.4754,  loss_tt=-1.3694,  loss_st1=-1.2999, loss_st2=-1.2983\n",
      "12-04 17:22:52 | (Train) | Epoch=2, batch=3400 loss=-1.2153, loss_ss=-0.4755,  loss_tt=-1.3539,  loss_st1=-1.2884, loss_st2=-1.2924\n",
      "12-04 17:25:59 | (Train) | Epoch=2, batch=3600 loss=-1.2204, loss_ss=-0.4769,  loss_tt=-1.3589,  loss_st1=-1.2944, loss_st2=-1.2975\n",
      "12-04 17:29:07 | (Train) | Epoch=2, batch=3800 loss=-1.2185, loss_ss=-0.4780,  loss_tt=-1.3696,  loss_st1=-1.2915, loss_st2=-1.2929\n",
      "Epoch: 2, loss: -4751.936552524567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:46<00:00, 5340.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-04 17:34:40 | (Train) | Epoch=3, batch=200 loss=-1.2198, loss_ss=-0.4820,  loss_tt=-1.3676,  loss_st1=-1.2917, loss_st2=-1.2955\n",
      "12-04 17:37:45 | (Train) | Epoch=3, batch=400 loss=-1.2245, loss_ss=-0.4830,  loss_tt=-1.3732,  loss_st1=-1.2971, loss_st2=-1.3000\n",
      "12-04 17:40:52 | (Train) | Epoch=3, batch=600 loss=-1.2340, loss_ss=-0.4849,  loss_tt=-1.3690,  loss_st1=-1.3110, loss_st2=-1.3105\n",
      "12-04 17:43:57 | (Train) | Epoch=3, batch=800 loss=-1.2341, loss_ss=-0.4860,  loss_tt=-1.3693,  loss_st1=-1.3098, loss_st2=-1.3116\n",
      "12-04 17:47:04 | (Train) | Epoch=3, batch=1000 loss=-1.2249, loss_ss=-0.4872,  loss_tt=-1.3681,  loss_st1=-1.2967, loss_st2=-1.3018\n",
      "12-04 17:50:09 | (Train) | Epoch=3, batch=1200 loss=-1.2318, loss_ss=-0.4878,  loss_tt=-1.3768,  loss_st1=-1.3076, loss_st2=-1.3059\n",
      "12-04 17:53:16 | (Train) | Epoch=3, batch=1400 loss=-1.2281, loss_ss=-0.4899,  loss_tt=-1.3760,  loss_st1=-1.2996, loss_st2=-1.3042\n",
      "12-04 17:56:24 | (Train) | Epoch=3, batch=1600 loss=-1.2302, loss_ss=-0.4916,  loss_tt=-1.3732,  loss_st1=-1.2990, loss_st2=-1.3103\n",
      "12-04 17:59:30 | (Train) | Epoch=3, batch=1800 loss=-1.2302, loss_ss=-0.4931,  loss_tt=-1.3737,  loss_st1=-1.3060, loss_st2=-1.3027\n",
      "12-04 18:02:36 | (Train) | Epoch=3, batch=2000 loss=-1.2359, loss_ss=-0.4953,  loss_tt=-1.3595,  loss_st1=-1.3136, loss_st2=-1.3123\n",
      "12-04 18:05:42 | (Train) | Epoch=3, batch=2200 loss=-1.2343, loss_ss=-0.4957,  loss_tt=-1.3800,  loss_st1=-1.3090, loss_st2=-1.3079\n",
      "12-04 18:08:49 | (Train) | Epoch=3, batch=2400 loss=-1.2314, loss_ss=-0.4962,  loss_tt=-1.3725,  loss_st1=-1.3024, loss_st2=-1.3089\n",
      "12-04 18:11:55 | (Train) | Epoch=3, batch=2600 loss=-1.2196, loss_ss=-0.4980,  loss_tt=-1.3795,  loss_st1=-1.2873, loss_st2=-1.2923\n",
      "12-04 18:15:03 | (Train) | Epoch=3, batch=2800 loss=-1.2315, loss_ss=-0.4989,  loss_tt=-1.3619,  loss_st1=-1.3019, loss_st2=-1.3116\n",
      "12-04 18:18:10 | (Train) | Epoch=3, batch=3000 loss=-1.2293, loss_ss=-0.4998,  loss_tt=-1.3654,  loss_st1=-1.3033, loss_st2=-1.3038\n",
      "12-04 18:21:18 | (Train) | Epoch=3, batch=3200 loss=-1.2285, loss_ss=-0.5007,  loss_tt=-1.3797,  loss_st1=-1.2987, loss_st2=-1.3025\n",
      "12-04 18:24:31 | (Train) | Epoch=3, batch=3400 loss=-1.2345, loss_ss=-0.5023,  loss_tt=-1.3759,  loss_st1=-1.3056, loss_st2=-1.3111\n",
      "12-04 18:27:37 | (Train) | Epoch=3, batch=3600 loss=-1.2289, loss_ss=-0.5019,  loss_tt=-1.3818,  loss_st1=-1.2950, loss_st2=-1.3064\n",
      "12-04 18:30:44 | (Train) | Epoch=3, batch=3800 loss=-1.2385, loss_ss=-0.5048,  loss_tt=-1.3716,  loss_st1=-1.3129, loss_st2=-1.3141\n",
      "Epoch: 3, loss: -4805.94983792305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:47<00:00, 5281.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-04 18:36:26 | (Train) | Epoch=4, batch=200 loss=-1.2278, loss_ss=-0.5062,  loss_tt=-1.3751,  loss_st1=-1.2982, loss_st2=-1.3009\n",
      "12-04 18:39:32 | (Train) | Epoch=4, batch=400 loss=-1.2453, loss_ss=-0.5063,  loss_tt=-1.3825,  loss_st1=-1.3161, loss_st2=-1.3251\n",
      "12-04 18:42:38 | (Train) | Epoch=4, batch=600 loss=-1.2403, loss_ss=-0.5076,  loss_tt=-1.3728,  loss_st1=-1.3133, loss_st2=-1.3173\n",
      "12-04 18:45:46 | (Train) | Epoch=4, batch=800 loss=-1.2402, loss_ss=-0.5079,  loss_tt=-1.3651,  loss_st1=-1.3130, loss_st2=-1.3193\n",
      "12-04 18:48:56 | (Train) | Epoch=4, batch=1000 loss=-1.2298, loss_ss=-0.5082,  loss_tt=-1.3723,  loss_st1=-1.3033, loss_st2=-1.3011\n",
      "12-04 18:52:06 | (Train) | Epoch=4, batch=1200 loss=-1.2355, loss_ss=-0.5102,  loss_tt=-1.3720,  loss_st1=-1.3078, loss_st2=-1.3103\n",
      "12-04 18:55:12 | (Train) | Epoch=4, batch=1400 loss=-1.2293, loss_ss=-0.5120,  loss_tt=-1.3660,  loss_st1=-1.2995, loss_st2=-1.3043\n",
      "12-04 18:58:17 | (Train) | Epoch=4, batch=1600 loss=-1.2322, loss_ss=-0.5125,  loss_tt=-1.3750,  loss_st1=-1.3049, loss_st2=-1.3037\n",
      "12-04 19:01:23 | (Train) | Epoch=4, batch=1800 loss=-1.2416, loss_ss=-0.5117,  loss_tt=-1.3798,  loss_st1=-1.3169, loss_st2=-1.3143\n",
      "12-04 19:04:31 | (Train) | Epoch=4, batch=2000 loss=-1.2284, loss_ss=-0.5133,  loss_tt=-1.3653,  loss_st1=-1.3054, loss_st2=-1.2960\n",
      "12-04 19:07:35 | (Train) | Epoch=4, batch=2200 loss=-1.2426, loss_ss=-0.5138,  loss_tt=-1.3753,  loss_st1=-1.3161, loss_st2=-1.3181\n",
      "12-04 19:10:39 | (Train) | Epoch=4, batch=2400 loss=-1.2363, loss_ss=-0.5145,  loss_tt=-1.3566,  loss_st1=-1.3131, loss_st2=-1.3099\n",
      "12-04 19:13:44 | (Train) | Epoch=4, batch=2600 loss=-1.2228, loss_ss=-0.5156,  loss_tt=-1.3687,  loss_st1=-1.2832, loss_st2=-1.3028\n",
      "12-04 19:16:49 | (Train) | Epoch=4, batch=2800 loss=-1.2375, loss_ss=-0.5172,  loss_tt=-1.3816,  loss_st1=-1.3072, loss_st2=-1.3119\n",
      "12-04 19:19:55 | (Train) | Epoch=4, batch=3000 loss=-1.2312, loss_ss=-0.5169,  loss_tt=-1.3821,  loss_st1=-1.2983, loss_st2=-1.3050\n",
      "12-04 19:23:00 | (Train) | Epoch=4, batch=3200 loss=-1.2381, loss_ss=-0.5166,  loss_tt=-1.3830,  loss_st1=-1.3078, loss_st2=-1.3124\n",
      "12-04 19:26:05 | (Train) | Epoch=4, batch=3400 loss=-1.2345, loss_ss=-0.5187,  loss_tt=-1.3832,  loss_st1=-1.3067, loss_st2=-1.3040\n",
      "12-04 19:29:11 | (Train) | Epoch=4, batch=3600 loss=-1.2384, loss_ss=-0.5183,  loss_tt=-1.3827,  loss_st1=-1.3096, loss_st2=-1.3111\n",
      "12-04 19:32:17 | (Train) | Epoch=4, batch=3800 loss=-1.2360, loss_ss=-0.5190,  loss_tt=-1.3789,  loss_st1=-1.3055, loss_st2=-1.3099\n",
      "Epoch: 4, loss: -4837.430552840233\n"
     ]
    }
   ],
   "source": [
    "model.train(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model.state_dict(), os.path.join(\"./clm_sf.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_model(\"clm_sf.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.load_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.341889072799386\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# n2v = models[-1]\n",
    "idxs = np.arange(len(network.line_graph.nodes))\n",
    "train_idx, test_idx = model_selection.train_test_split(idxs, test_size=0.2, random_state=69)\n",
    "y = np.array([network.gdf_edges.loc[n][\"highway_enc\"] for n in network.line_graph.nodes])\n",
    "\n",
    "# for m, e in models:\n",
    "    # m.train(epochs=e)\n",
    "    # zn = m.load_emb()\n",
    "    # zcn = np.concatenate((zn, z2), axis=1)\n",
    "    # zct = np.concatenate((zn, z3), axis=1)\n",
    "    # zcnn = np.concatenate((zn, z4), axis=1)\n",
    "    # zctn = np.concatenate((zn, z5), axis=1)\n",
    "    # X = z # embedding for each node\n",
    "eva = [z] # gtc.load_emb(), gae_emb, rand_emb\n",
    "for X in eva:\n",
    "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    lm = linear_model.LogisticRegression(multi_class=\"multinomial\", max_iter=1000)\n",
    "    # lm.fit(X_train, y_train)\n",
    "    scorer = make_scorer(metrics.f1_score, average=\"macro\")\n",
    "    print(np.mean(cross_val_score(estimator=lm, X=X, y=y, scoring=scorer, cv=5)))\n",
    "    #print(metrics.classification_report(y_test, lm.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('road')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "088070de2c6b4023b2f7ae556c412f86bcd02589c7bdb3766a0caf3cf4813fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
