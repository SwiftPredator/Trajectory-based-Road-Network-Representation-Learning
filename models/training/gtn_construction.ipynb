{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from generator import RoadNetwork, Trajectory\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from models import GTNModel, GTCModel, Traj2VecModel, Node2VecModel, GAEModel, GCNEncoder, SRN2VecModel\n",
    "from evaluation.tasks import TravelTimeEstimation, NextLocationPrediciton, DestinationPrediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a28d0719fb45708b9a127bfe71ce48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1544234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b39d9628d145f19b4330cc2eb9c795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1544234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df3d2ed28214fcf821ceaba9e255dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1544234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51f01ae22f3a4a049480bf73a171835c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1544234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f7838e8d0c42dd86b48e6ee217c5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1544234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network = RoadNetwork()\n",
    "network.load(\"../../osm_data/porto\")\n",
    "trajectory = Trajectory(\"../../datasets/trajectories/Porto/road_segment_map_final.csv\", nrows=10000000).generate_TTE_datatset()\n",
    "traj_features = pd.read_csv(\"../../datasets/trajectories/Porto/speed_features_unnormalized.csv\")\n",
    "traj_features.set_index([\"u\", \"v\", \"key\"], inplace=True)\n",
    "traj_features.fillna(0, inplace=True)\n",
    "\n",
    "data_roadclf = network.generate_road_segment_pyg_dataset(include_coords=True, drop_labels=[\"highway_enc\"], traj_data=None)\n",
    "data_rest = network.generate_road_segment_pyg_dataset(include_coords=True, traj_data=None)\n",
    "\n",
    "adj = np.loadtxt(\"./gtn_precalc_adj/traj_adj_k_2.gz\")\n",
    "# adj_k3 = np.loadtxt(\"./gtn_precalc_adj/traj_adj_k_3.gz\")\n",
    "adj_sample = np.loadtxt(\"./gtn_precalc_adj/traj_adj_k_1_False_no_selfloops_smoothed.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(trajectory, test_size=0.1, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create init emb from gtc and traj2vec concat\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "traj2vec = Traj2VecModel(data_rest, network, adj, device=device, emb_dim=128)\n",
    "traj2vec.load_model(\"../model_states/traj2vec/model_base.pt\")\n",
    "gtc = GTCModel(data_rest, device, network, None, adj=adj)\n",
    "gtc.load_model(\"../model_states/gtc/model_base.pt\")\n",
    "node2vec = Node2VecModel(data_rest, device=device, q=4, p=1)\n",
    "node2vec.load_model(\"../model_states/node2vec/model_base.pt\")\n",
    "gae = GAEModel(data_rest, device=device, encoder=GCNEncoder, emb_dim=128)\n",
    "gae.load_model(\"../model_states/gaegcn/model_base.pt\")\n",
    "srn = SRN2VecModel(None, device, network, remove_highway_label=False)\n",
    "srn.load_dataset(\"./srn2vec-traindata.json\")\n",
    "srn.load_model(\"../model_states/srn2vec/model_base.pt\")\n",
    "\n",
    "\n",
    "init_emb = torch.Tensor(np.concatenate([gtc.load_emb(), traj2vec.load_emb()], axis=1))\n",
    "add_emb = torch.Tensor(gtc.load_emb() + traj2vec.load_emb())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right insert\n"
     ]
    }
   ],
   "source": [
    "# init GTN Model\n",
    "model = GTNModel(data_rest, device, network, train, traj_features, init_emb, adj_sample, batch_size=512, hidden_dim=512)\n",
    "model.load_model(\"../model_states/gtn/model_base_gtc_k2_10e_noutil_autoreg.pt\")\n",
    "# gtn_add = GTNModel(data_rest, device, network, train, traj_features, add_emb, adj_sample, batch_size=32, emb_dim=128, hidden_dim=128)\n",
    "# gtn_add.load_model(\"../model_states/gtn/model_base_add.pt\")\n",
    "# gtn_con = GTNModel(data_rest, device, network, train, traj_features, init_emb, adj_sample, batch_size=32)\n",
    "# gtn_con.load_model(\"../model_states/gtn/model_base_concat.pt\")\n",
    "# gtn_trans = GTNModel(data_rest, device, network, train, traj_features, None, adj_sample, batch_size=32, emb_dim=128, hidden_dim=128)\n",
    "# gtn_trans.load_model(\"../model_states/gtn/model_base_only_trans.pt\")\n",
    "# gtn_con_25 = GTNModel(data_rest, device, network, train, traj_features, init_emb, adj_sample, batch_size=32)\n",
    "# gtn_con_25.load_model(\"../model_states/gtn/model_base.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right insert\n"
     ]
    }
   ],
   "source": [
    "model2 = GTNModel(data_rest, device, network, train, traj_features, init_emb, adj_sample, batch_size=512, hidden_dim=256)\n",
    "model2.load_model(\"../model_states/gtn/model_base_gtc_k2_10e_noutil_noautoreg_32_batch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, iter 0 loss: 1.0847514867782593, masked traj loss 0.577, judge traj loss 0.508\n",
      "Epoch: 0, iter 10 loss: 1.0973563194274902, masked traj loss 0.599, judge traj loss 0.498\n",
      "Epoch: 0, iter 20 loss: 1.1238110065460205, masked traj loss 0.631, judge traj loss 0.493\n",
      "Epoch 0 avg_loss=1.3245706991715864 total_acc= 73.14285714285714\n",
      "Epoch: 1, iter 0 loss: 1.3249919414520264, masked traj loss 0.871, judge traj loss 0.454\n",
      "Epoch: 1, iter 10 loss: 1.2418620586395264, masked traj loss 0.756, judge traj loss 0.486\n",
      "Epoch: 1, iter 20 loss: 1.712965726852417, masked traj loss 1.154, judge traj loss 0.559\n",
      "Epoch 1 avg_loss=1.3454584370959888 total_acc= 74.57142857142857\n"
     ]
    }
   ],
   "source": [
    "model.train(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = model.load_emb()[2:]\n",
    "# emb2 = model2.load_emb()[2:]\n",
    "emb3 = model2.load_emb()[2:]\n",
    "# emb2 = model2.load_emb()\n",
    "# emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_emb = node2vec.load_emb()\n",
    "gae_emb = gae.load_emb()\n",
    "srn_emb = srn.load_emb()\n",
    "n2v_emb = node2vec.load_emb()\n",
    "# gtn_add_emb = gtn_add.load_emb()\n",
    "# gtn_concat_emb = gtn_con.load_emb()\n",
    "# gtn_only_trans_emb = gtn_trans.load_emb()\n",
    "# gtn_con_25_emb = gtn_con_25.load_emb()\n",
    "rand_emb = np.random.randn(*emb3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model.state_dict(), os.path.join(\"../model_states/gtn/\" + \"/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7304362871951083\n",
      "0.8186748315183644\n",
      "0.85607575965873\n",
      "0.6261817759851643\n",
      "0.8517336485185958\n",
      "0.054525170059597936\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "idxs = np.arange(len(network.line_graph.nodes))\n",
    "# train_idx, test_idx = model_selection.train_test_split(idxs, test_size=0.2, random_state=69)\n",
    "y = np.array([network.gdf_edges.loc[n][\"highway_enc\"] for n in network.line_graph.nodes])\n",
    "\n",
    "# z = emb\n",
    "emb_full = np.concatenate((init_emb, emb3), axis=1)\n",
    "# emb_full2 = np.concatenate((init_emb, emb2), axis=1)\n",
    "# emb_gtc = np.concatenate((gtc.load_emb(), emb), axis=1)\n",
    "# zadd = np.add(emb, init_emb)\n",
    "# zcnn = np.concatenate((zn, z4), axis=1)\n",
    "# zctn = np.concatenate((zn, z5), axis=1)\n",
    "# X = z # embedding for each node\n",
    "eva = [emb3, emb_full, init_emb, gae_emb, srn_emb, rand_emb]\n",
    "for X in eva:\n",
    "    # X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    lm = linear_model.LogisticRegression(multi_class=\"multinomial\", max_iter=1000)\n",
    "    # lm.fit(X_train, y_train)\n",
    "    scorer = make_scorer(metrics.f1_score, average=\"macro\")\n",
    "    print(np.mean(cross_val_score(estimator=lm, X=X, y=y, scoring=scorer, cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.982448581991415\n",
      "12.725772435475433\n",
      "15.861414443131949\n",
      "14.285923659867086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "tf = pd.read_csv(\"../../datasets/trajectories/Porto/speed_features_unnormalized.csv\")\n",
    "tf.set_index([\"u\", \"v\", \"key\"], inplace=True)\n",
    "map_id = {j: i for i, j in enumerate(network.line_graph.nodes)}\n",
    "tf[\"idx\"] = tf.index.map(map_id)\n",
    "tf.sort_values(by=\"idx\", axis=0, inplace=True)\n",
    "\n",
    "idxs = np.arange(len(network.line_graph.nodes))\n",
    "train_idx, test_idx = model_selection.train_test_split(idxs, test_size=0.2, random_state=69)\n",
    "\n",
    "y = tf[\"avg_speed\"]\n",
    "y.fillna(0, inplace=True)\n",
    "y = y.round(2)\n",
    "y = y.values\n",
    "\n",
    "# z = emb\n",
    "# zct = np.concatenate((init_emb, emb), axis=1)\n",
    "# zadd = np.add(emb, init_emb)\n",
    "# emb_full = np.concatenate((init_emb, emb), axis=1)\n",
    "# emb_full2 = np.concatenate((init_emb, emb2), axis=1)\n",
    "# emb_gtc = np.concatenate((gtc.load_emb(), emb), axis=1)\n",
    "eva = [emb3, init_emb, srn_emb, n2v_emb]\n",
    "for X in eva:\n",
    "    decoder = MLPRegressor(hidden_layer_sizes=(1024), random_state=88, max_iter=30)\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    # decoder.fit(X_train, y_train)\n",
    "    scorer = make_scorer(metrics.mean_absolute_error)\n",
    "    print(np.mean(cross_val_score(estimator=decoder, X=X, y=y, scoring=scorer, cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAE': 80.1711030956158}\n",
      "{'MAE': 79.77212380921698}\n",
      "{'MAE': 77.8282006005686}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/gtn_construction.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal05/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/gtn_construction.ipynb#ch0000013vscode-remote?line=20'>21</a>\u001b[0m eva \u001b[39m=\u001b[39m [emb3, zct, n2v_emb, srn_emb]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal05/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/gtn_construction.ipynb#ch0000013vscode-remote?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m eva:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bpascal05/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/gtn_construction.ipynb#ch0000013vscode-remote?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(travel_time_est\u001b[39m.\u001b[39;49mevaluate(X))\n",
      "File \u001b[0;32m/dstore/home/pheinemeyer/Road-Network-Embedding-Generator/evaluation/tasks/travel_time.py:61\u001b[0m, in \u001b[0;36mTravelTimeEstimation.evaluate\u001b[0;34m(self, emb)\u001b[0m\n\u001b[1;32m     56\u001b[0m model \u001b[39m=\u001b[39m TTE_LSTM(\n\u001b[1;32m     57\u001b[0m     device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice, emb_dim\u001b[39m=\u001b[39memb\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     60\u001b[0m \u001b[39m# train on x trajectories\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(loader\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loader, emb\u001b[39m=\u001b[39;49memb, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs)\n\u001b[1;32m     63\u001b[0m \u001b[39m# eval on rest\u001b[39;00m\n\u001b[1;32m     64\u001b[0m yh, y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(loader\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_loader, emb\u001b[39m=\u001b[39memb)\n",
      "File \u001b[0;32m/dstore/home/pheinemeyer/Road-Network-Embedding-Generator/evaluation/tasks/travel_time.py:202\u001b[0m, in \u001b[0;36mTTE_LSTM.train_model\u001b[0;34m(self, loader, emb, epochs)\u001b[0m\n\u001b[1;32m    199\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(yh\u001b[39m.\u001b[39msqueeze(), y)\n\u001b[1;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 202\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    204\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "travel_time_est = TravelTimeEstimation(\n",
    "    traj_dataset=test,\n",
    "    network=network,\n",
    "    device=device,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    seed=88,\n",
    ")\n",
    "# travel_time_est.register_metric(\n",
    "#     name=\"MSE\", metric_func=metrics.mean_squared_error, args={}\n",
    "# )\n",
    "travel_time_est.register_metric(\n",
    "    name=\"MAE\", metric_func=metrics.mean_absolute_error, args={}\n",
    ")\n",
    "\n",
    "# z = emb\n",
    "zct = np.concatenate((init_emb, emb3), axis=1)\n",
    "# zadd = np.add(emb, init_emb)\n",
    "# emb_full = np.concatenate((init_emb, emb), axis=1)\n",
    "# emb_gtc = np.concatenate((gtc.load_emb(), emb), axis=1)\n",
    "eva = [emb3, zct, n2v_emb, srn_emb]\n",
    "for X in eva:\n",
    "    print(travel_time_est.evaluate(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df8178490ff4ddb9f763455981bc679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/108096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9193ddff9662488fa3af5cd359fc7a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/108096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b0c4591ba34f21807321c651ad181b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/46328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff1897409584a0ba060e1c98218ebd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/46328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 0: 85.58177826559755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 1: 59.24939918292345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 2: 49.3140873835637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 3: 44.69477894122784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 4: 41.17308108312844\n",
      "{'accuracy': 0.46643498532205147}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53512458556c4b61907bd406c163f6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/108096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b337d7bafeef4b23baf2e232379e86d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/108096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574c7ab467554ee8a22b3d7da02ddfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/46328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30311f4be844d9cab0801d048fb9979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/46328 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 0: 107.92889009735288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 1: 108.84033501980572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/gtn_construction.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal05/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/gtn_construction.ipynb#ch0000014vscode-remote?line=21'>22</a>\u001b[0m eva \u001b[39m=\u001b[39m [emb3, n2v_emb, srn_emb]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bpascal05/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/gtn_construction.ipynb#ch0000014vscode-remote?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m eva:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bpascal05/home/pheinemeyer/Road-Network-Embedding-Generator/models/training/gtn_construction.ipynb#ch0000014vscode-remote?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(nextlocation_pred\u001b[39m.\u001b[39;49mevaluate(X))\n",
      "File \u001b[0;32m/dstore/home/pheinemeyer/Road-Network-Embedding-Generator/evaluation/tasks/next_location.py:76\u001b[0m, in \u001b[0;36mNextLocationPrediciton.evaluate\u001b[0;34m(self, emb, coord_sys)\u001b[0m\n\u001b[1;32m     58\u001b[0m model \u001b[39m=\u001b[39m NL_LSTM(\n\u001b[1;32m     59\u001b[0m     out_dim\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\n\u001b[1;32m     60\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mline_graph\u001b[39m.\u001b[39mnodes\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[39m# train on x trajectories\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(loader\u001b[39m=\u001b[39;49mtrain_loader, emb\u001b[39m=\u001b[39;49memb, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs)\n\u001b[1;32m     78\u001b[0m \u001b[39m# eval on test set using distance loss\u001b[39;00m\n\u001b[1;32m     79\u001b[0m yh, y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(loader\u001b[39m=\u001b[39meval_loader, emb\u001b[39m=\u001b[39memb)\n",
      "File \u001b[0;32m/dstore/home/pheinemeyer/Road-Network-Embedding-Generator/evaluation/tasks/next_location.py:297\u001b[0m, in \u001b[0;36mNL_LSTM.train_model\u001b[0;34m(self, loader, emb, epochs)\u001b[0m\n\u001b[1;32m    295\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(yh\u001b[39m.\u001b[39msqueeze(), y)\n\u001b[1;32m    296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 297\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    299\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/road/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nextlocation_pred = NextLocationPrediciton(\n",
    "    traj_dataset=test,\n",
    "    network=network,\n",
    "    device=device,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    seed=88,\n",
    ")\n",
    "\n",
    "nextlocation_pred.register_metric(\n",
    "    name=\"accuracy\",\n",
    "    metric_func=metrics.accuracy_score,\n",
    "    args={\"normalize\": True},\n",
    ")\n",
    "\n",
    "# z = emb\n",
    "# zctn = np.concatenate((init_emb, emb), axis=1)\n",
    "# zadd = np.add(emb, init_emb)\n",
    "# emb_full = np.concatenate((init_emb, emb), axis=1)\n",
    "# emb_full2 = np.concatenate((init_emb, emb2), axis=1)\n",
    "# emb_gtc = np.concatenate((gtc.load_emb(), emb), axis=1)\n",
    "eva = [emb3, n2v_emb, srn_emb]\n",
    "for X in eva:\n",
    "    print(nextlocation_pred.evaluate(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005256c0f13a43dd81986c5e55ef444b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/370616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c482b2db963f4d9998c2c843d6ad3a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/370616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25a4184345a4034998495e81c6fc440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/92655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0256944db3f4dc4a23a66b7ed1659b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/92655 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 0: 5.167947872814553\n",
      "Average training loss in episode 1: 4.283290731890426\n",
      "Average training loss in episode 2: 4.077435616959525\n",
      "Average training loss in episode 3: 3.976194605313612\n",
      "Average training loss in episode 4: 3.916674303745038\n",
      "{'accuracy': 0.1951324807080028}\n",
      "Average training loss in episode 0: 5.240711502525029\n",
      "Average training loss in episode 1: 4.168358718986669\n",
      "Average training loss in episode 2: 3.8998264221690637\n",
      "Average training loss in episode 3: 3.766718561231102\n",
      "Average training loss in episode 4: 3.6838043355151435\n",
      "{'accuracy': 0.21757055744428255}\n"
     ]
    }
   ],
   "source": [
    "dest_pred = DestinationPrediciton(\n",
    "    traj_dataset=test,\n",
    "    network=network,\n",
    "    device=device,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    seed=88,\n",
    ")\n",
    "\n",
    "dest_pred.register_metric(\n",
    "    name=\"accuracy\",\n",
    "    metric_func=metrics.accuracy_score,\n",
    "    args={\"normalize\": True},\n",
    ")\n",
    "\n",
    "# emb_full = np.concatenate((init_emb, emb), axis=1)\n",
    "# emb_full2 = np.concatenate((init_emb, emb2), axis=1)\n",
    "# emb_gtc = np.concatenate((gtc.load_emb(), emb), axis=1)\n",
    "eva = [emb3, n2v_emb]\n",
    "for X in eva:\n",
    "    print(dest_pred.evaluate(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('road')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "088070de2c6b4023b2f7ae556c412f86bcd02589c7bdb3766a0caf3cf4813fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
