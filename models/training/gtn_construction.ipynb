{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from generator import RoadNetwork, Trajectory\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from models import GTNModel, GTCModel, Traj2VecModel, Node2VecModel, GAEModel, GCNEncoder, SRN2VecModel\n",
    "from evaluation.tasks import TravelTimeEstimation, NextLocationPrediciton, DestinationPrediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6379c1cfa3f846a48ad42fa12fe7df94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29f8f2bb26841509a1e0a86c10b7bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b8d5d78fd24ce18d69ececec451401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc5482d81804d8197a99aa7e38a3c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c811574e0cf940dc9b4cd9768efb3b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network = RoadNetwork()\n",
    "network.load(\"../../osm_data/porto\")\n",
    "trajectory = Trajectory(\"../../datasets/trajectories/Porto/road_segment_map_final.csv\", nrows=500000).generate_TTE_datatset()\n",
    "traj_features = pd.read_csv(\"../../datasets/trajectories/Porto/speed_features_unnormalized.csv\")\n",
    "traj_features.set_index([\"u\", \"v\", \"key\"], inplace=True)\n",
    "traj_features.fillna(0, inplace=True)\n",
    "\n",
    "data_roadclf = network.generate_road_segment_pyg_dataset(include_coords=True, drop_labels=[\"highway_enc\"], traj_data=None)\n",
    "data_rest = network.generate_road_segment_pyg_dataset(include_coords=True, traj_data=None)\n",
    "\n",
    "adj = np.loadtxt(\"./gtn_precalc_adj/traj_adj_k_1.gz\")\n",
    "adj_sample = np.loadtxt(\"./gtn_precalc_adj/traj_adj_k_1_False_no_selfloops_smoothed.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(trajectory, test_size=0.3, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create init emb from gtc and traj2vec concat\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "traj2vec = Traj2VecModel(data_roadclf, network, adj, device=device, emb_dim=128)\n",
    "traj2vec.load_model(\"../model_states/traj2vec/model_base.pt\")\n",
    "gtc = GTCModel(data_roadclf, device, network, None, adj=adj)\n",
    "gtc.load_model(\"../model_states/gtc/model_noroad.pt\")\n",
    "node2vec = Node2VecModel(data_roadclf, device=device, q=4, p=1)\n",
    "node2vec.load_model(\"../model_states/node2vec/model_base.pt\")\n",
    "gae = GAEModel(data_roadclf, device=device, encoder=GCNEncoder, emb_dim=128)\n",
    "gae.load_model(\"../model_states/gaegcn/model_noroad.pt\")\n",
    "srn = SRN2VecModel(None, device, network, remove_highway_label=True)\n",
    "srn.load_dataset(\"./srn2vec-traindata.json\")\n",
    "srn.load_model(\"../model_states/srn2vec/model_noroad.pt\")\n",
    "\n",
    "\n",
    "init_emb = torch.Tensor(np.concatenate([gtc.load_emb(), traj2vec.load_emb()], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init GTN Model\n",
    "model = GTNModel(data_roadclf, device, network, train, traj_features, init_emb, adj_sample, batch_size=32)\n",
    "# model.load_model(\"../model_states/gtn/model_noroad.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, iter 0 loss: 711587456.0, masked traj loss 0.847, judge traj loss 0.689, util loss 711587456.000\n",
      "Epoch: 0, iter 500 loss: 603249408.0, masked traj loss 0.573, judge traj loss 0.694, util loss 603249408.000\n",
      "Epoch: 0, iter 1000 loss: 566962176.0, masked traj loss 0.585, judge traj loss 0.688, util loss 566962176.000\n",
      "Epoch: 0, iter 1500 loss: 478014816.0, masked traj loss 0.994, judge traj loss 0.686, util loss 478014816.000\n",
      "Epoch: 0, iter 2000 loss: 665787072.0, masked traj loss 0.922, judge traj loss 0.687, util loss 665787072.000\n",
      "Epoch: 0, iter 2500 loss: 605643904.0, masked traj loss 0.832, judge traj loss 0.685, util loss 605643904.000\n",
      "Epoch: 0, iter 3000 loss: 731862592.0, masked traj loss 0.656, judge traj loss 0.691, util loss 731862592.000\n",
      "Epoch: 0, iter 3500 loss: 1114920448.0, masked traj loss 0.753, judge traj loss 0.596, util loss 1114920448.000\n",
      "Epoch: 0, iter 4000 loss: 504528704.0, masked traj loss 1.192, judge traj loss 0.639, util loss 504528704.000\n",
      "Epoch: 0, iter 4500 loss: 878870656.0, masked traj loss 0.953, judge traj loss 0.586, util loss 878870656.000\n",
      "Epoch: 0, iter 5000 loss: 740835776.0, masked traj loss 0.712, judge traj loss 0.642, util loss 740835776.000\n",
      "Epoch: 0, iter 5500 loss: 898293120.0, masked traj loss 1.106, judge traj loss 0.584, util loss 898293120.000\n",
      "Epoch: 0, iter 6000 loss: 858778112.0, masked traj loss 0.978, judge traj loss 0.539, util loss 858778112.000\n",
      "Epoch: 0, iter 6500 loss: 405425600.0, masked traj loss 0.568, judge traj loss 0.590, util loss 405425600.000\n",
      "Epoch: 0, iter 7000 loss: 409150432.0, masked traj loss 0.530, judge traj loss 0.499, util loss 409150432.000\n",
      "Epoch: 0, iter 7500 loss: 567042048.0, masked traj loss 0.637, judge traj loss 0.515, util loss 567042048.000\n",
      "Epoch: 0, iter 8000 loss: 340656416.0, masked traj loss 0.866, judge traj loss 0.354, util loss 340656416.000\n",
      "Epoch: 0, iter 8500 loss: 420281952.0, masked traj loss 0.721, judge traj loss 0.596, util loss 420281952.000\n",
      "Epoch: 0, iter 9000 loss: 319536256.0, masked traj loss 0.583, judge traj loss 0.528, util loss 319536256.000\n",
      "Epoch: 0, iter 9500 loss: 314792960.0, masked traj loss 0.441, judge traj loss 0.577, util loss 314792960.000\n",
      "Epoch: 0, iter 10000 loss: 273210624.0, masked traj loss 0.575, judge traj loss 0.359, util loss 273210624.000\n",
      "Epoch: 0, iter 10500 loss: 447967296.0, masked traj loss 0.536, judge traj loss 0.356, util loss 447967296.000\n",
      "Epoch 0 avg_loss=595975191.9283233 total_acc= 64.79314285714285\n",
      "Epoch: 1, iter 0 loss: 517688928.0, masked traj loss 0.799, judge traj loss 0.383, util loss 517688928.000\n",
      "Epoch: 1, iter 500 loss: 699645312.0, masked traj loss 0.562, judge traj loss 0.500, util loss 699645312.000\n",
      "Epoch: 1, iter 1000 loss: 308954432.0, masked traj loss 0.816, judge traj loss 0.411, util loss 308954432.000\n",
      "Epoch: 1, iter 1500 loss: 545935232.0, masked traj loss 0.757, judge traj loss 0.403, util loss 545935232.000\n",
      "Epoch: 1, iter 2000 loss: 720175936.0, masked traj loss 0.355, judge traj loss 0.443, util loss 720175936.000\n",
      "Epoch: 1, iter 2500 loss: 490072992.0, masked traj loss 0.816, judge traj loss 0.432, util loss 490072992.000\n",
      "Epoch: 1, iter 3000 loss: 604887744.0, masked traj loss 0.984, judge traj loss 0.398, util loss 604887744.000\n",
      "Epoch: 1, iter 3500 loss: 509543296.0, masked traj loss 0.941, judge traj loss 0.534, util loss 509543296.000\n",
      "Epoch: 1, iter 4000 loss: 611048704.0, masked traj loss 0.822, judge traj loss 0.313, util loss 611048704.000\n",
      "Epoch: 1, iter 4500 loss: 625144704.0, masked traj loss 0.503, judge traj loss 0.330, util loss 625144704.000\n",
      "Epoch: 1, iter 5000 loss: 438224864.0, masked traj loss 0.579, judge traj loss 0.458, util loss 438224864.000\n",
      "Epoch: 1, iter 5500 loss: 543847744.0, masked traj loss 0.600, judge traj loss 0.392, util loss 543847744.000\n",
      "Epoch: 1, iter 6000 loss: 570024960.0, masked traj loss 0.871, judge traj loss 0.307, util loss 570024960.000\n",
      "Epoch: 1, iter 6500 loss: 438107264.0, masked traj loss 0.848, judge traj loss 0.467, util loss 438107264.000\n",
      "Epoch: 1, iter 7000 loss: 568670784.0, masked traj loss 0.664, judge traj loss 0.266, util loss 568670784.000\n",
      "Epoch: 1, iter 7500 loss: 561732224.0, masked traj loss 0.397, judge traj loss 0.480, util loss 561732224.000\n",
      "Epoch: 1, iter 8000 loss: 422426176.0, masked traj loss 0.746, judge traj loss 0.524, util loss 422426176.000\n",
      "Epoch: 1, iter 8500 loss: 372476256.0, masked traj loss 0.689, judge traj loss 0.782, util loss 372476256.000\n",
      "Epoch: 1, iter 9000 loss: 736543616.0, masked traj loss 0.546, judge traj loss 0.405, util loss 736543616.000\n",
      "Epoch: 1, iter 9500 loss: 604933376.0, masked traj loss 0.569, judge traj loss 0.332, util loss 604933376.000\n",
      "Epoch: 1, iter 10000 loss: 589426944.0, masked traj loss 0.362, judge traj loss 0.470, util loss 589426944.000\n",
      "Epoch: 1, iter 10500 loss: 680833280.0, masked traj loss 0.542, judge traj loss 0.314, util loss 680833280.000\n",
      "Epoch 1 avg_loss=548356800.4125068 total_acc= 81.33314285714286\n",
      "Epoch: 2, iter 0 loss: 404301056.0, masked traj loss 0.411, judge traj loss 0.363, util loss 404301056.000\n",
      "Epoch: 2, iter 500 loss: 460213120.0, masked traj loss 0.538, judge traj loss 0.247, util loss 460213120.000\n",
      "Epoch: 2, iter 1000 loss: 675279616.0, masked traj loss 0.872, judge traj loss 0.540, util loss 675279616.000\n",
      "Epoch: 2, iter 1500 loss: 408660576.0, masked traj loss 0.670, judge traj loss 0.736, util loss 408660576.000\n",
      "Epoch: 2, iter 2000 loss: 677552128.0, masked traj loss 0.626, judge traj loss 0.468, util loss 677552128.000\n",
      "Epoch: 2, iter 2500 loss: 572215872.0, masked traj loss 0.810, judge traj loss 0.332, util loss 572215872.000\n",
      "Epoch: 2, iter 3000 loss: 522807392.0, masked traj loss 0.770, judge traj loss 0.422, util loss 522807392.000\n",
      "Epoch: 2, iter 3500 loss: 322694368.0, masked traj loss 0.667, judge traj loss 0.470, util loss 322694368.000\n",
      "Epoch: 2, iter 4000 loss: 474255232.0, masked traj loss 0.651, judge traj loss 0.423, util loss 474255232.000\n",
      "Epoch: 2, iter 4500 loss: 497831776.0, masked traj loss 0.834, judge traj loss 0.364, util loss 497831776.000\n",
      "Epoch: 2, iter 5000 loss: 583342592.0, masked traj loss 0.748, judge traj loss 0.483, util loss 583342592.000\n",
      "Epoch: 2, iter 5500 loss: 289454432.0, masked traj loss 0.622, judge traj loss 0.540, util loss 289454432.000\n",
      "Epoch: 2, iter 6000 loss: 321380224.0, masked traj loss 0.791, judge traj loss 0.492, util loss 321380224.000\n",
      "Epoch: 2, iter 6500 loss: 574543168.0, masked traj loss 0.644, judge traj loss 0.414, util loss 574543168.000\n",
      "Epoch: 2, iter 7000 loss: 324270016.0, masked traj loss 0.934, judge traj loss 0.455, util loss 324270016.000\n",
      "Epoch: 2, iter 7500 loss: 642366528.0, masked traj loss 0.671, judge traj loss 0.490, util loss 642366528.000\n",
      "Epoch: 2, iter 8000 loss: 318816128.0, masked traj loss 0.614, judge traj loss 0.501, util loss 318816128.000\n",
      "Epoch: 2, iter 8500 loss: 566097664.0, masked traj loss 0.702, judge traj loss 0.470, util loss 566097664.000\n",
      "Epoch: 2, iter 9000 loss: 629265536.0, masked traj loss 0.226, judge traj loss 0.316, util loss 629265536.000\n",
      "Epoch: 2, iter 9500 loss: 465426976.0, masked traj loss 0.787, judge traj loss 0.553, util loss 465426976.000\n",
      "Epoch: 2, iter 10000 loss: 345468192.0, masked traj loss 0.577, judge traj loss 0.400, util loss 345468192.000\n",
      "Epoch: 2, iter 10500 loss: 337245696.0, masked traj loss 0.555, judge traj loss 0.393, util loss 337245696.000\n",
      "Epoch 2 avg_loss=498473479.94514537 total_acc= 80.67885714285714\n",
      "Epoch: 3, iter 0 loss: 370834112.0, masked traj loss 0.561, judge traj loss 0.393, util loss 370834112.000\n",
      "Epoch: 3, iter 500 loss: 579430400.0, masked traj loss 0.412, judge traj loss 0.329, util loss 579430400.000\n",
      "Epoch: 3, iter 1000 loss: 562690112.0, masked traj loss 0.559, judge traj loss 0.404, util loss 562690112.000\n",
      "Epoch: 3, iter 1500 loss: 650886080.0, masked traj loss 0.573, judge traj loss 0.408, util loss 650886080.000\n",
      "Epoch: 3, iter 2000 loss: 388752960.0, masked traj loss 0.713, judge traj loss 0.370, util loss 388752960.000\n",
      "Epoch: 3, iter 2500 loss: 486104064.0, masked traj loss 0.594, judge traj loss 0.417, util loss 486104064.000\n",
      "Epoch: 3, iter 3000 loss: 376067712.0, masked traj loss 0.419, judge traj loss 0.333, util loss 376067712.000\n",
      "Epoch: 3, iter 3500 loss: 411651232.0, masked traj loss 0.434, judge traj loss 0.250, util loss 411651232.000\n",
      "Epoch: 3, iter 4000 loss: 574360064.0, masked traj loss 0.471, judge traj loss 0.408, util loss 574360064.000\n",
      "Epoch: 3, iter 4500 loss: 545575808.0, masked traj loss 0.565, judge traj loss 0.292, util loss 545575808.000\n",
      "Epoch: 3, iter 5000 loss: 343521920.0, masked traj loss 0.473, judge traj loss 0.168, util loss 343521920.000\n",
      "Epoch: 3, iter 5500 loss: 591485312.0, masked traj loss 0.417, judge traj loss 0.163, util loss 591485312.000\n",
      "Epoch: 3, iter 6000 loss: 459385312.0, masked traj loss 0.544, judge traj loss 0.201, util loss 459385312.000\n",
      "Epoch: 3, iter 6500 loss: 505632128.0, masked traj loss 0.490, judge traj loss 0.271, util loss 505632128.000\n",
      "Epoch: 3, iter 7000 loss: 333053344.0, masked traj loss 0.534, judge traj loss 0.240, util loss 333053344.000\n",
      "Epoch: 3, iter 7500 loss: 584297728.0, masked traj loss 0.538, judge traj loss 0.342, util loss 584297728.000\n",
      "Epoch: 3, iter 8000 loss: 555953472.0, masked traj loss 0.644, judge traj loss 0.424, util loss 555953472.000\n",
      "Epoch: 3, iter 8500 loss: 382783200.0, masked traj loss 0.618, judge traj loss 0.427, util loss 382783200.000\n",
      "Epoch: 3, iter 9000 loss: 489987584.0, masked traj loss 0.357, judge traj loss 0.296, util loss 489987584.000\n",
      "Epoch: 3, iter 9500 loss: 439517440.0, masked traj loss 0.406, judge traj loss 0.428, util loss 439517440.000\n",
      "Epoch: 3, iter 10000 loss: 232473952.0, masked traj loss 0.382, judge traj loss 0.230, util loss 232473952.000\n",
      "Epoch: 3, iter 10500 loss: 504928480.0, masked traj loss 0.431, judge traj loss 0.168, util loss 504928480.000\n",
      "Epoch 3 avg_loss=455830322.5679283 total_acc= 84.21257142857142\n",
      "Epoch: 4, iter 0 loss: 706654336.0, masked traj loss 0.829, judge traj loss 0.605, util loss 706654336.000\n",
      "Epoch: 4, iter 500 loss: 317934464.0, masked traj loss 0.672, judge traj loss 0.319, util loss 317934464.000\n",
      "Epoch: 4, iter 1000 loss: 395556032.0, masked traj loss 0.550, judge traj loss 0.485, util loss 395556032.000\n",
      "Epoch: 4, iter 1500 loss: 448087104.0, masked traj loss 0.573, judge traj loss 0.250, util loss 448087104.000\n",
      "Epoch: 4, iter 2000 loss: 335755616.0, masked traj loss 0.541, judge traj loss 0.359, util loss 335755616.000\n",
      "Epoch: 4, iter 2500 loss: 366869888.0, masked traj loss 0.480, judge traj loss 0.311, util loss 366869888.000\n",
      "Epoch: 4, iter 3000 loss: 477286560.0, masked traj loss 0.627, judge traj loss 0.394, util loss 477286560.000\n",
      "Epoch: 4, iter 3500 loss: 756063232.0, masked traj loss 0.558, judge traj loss 0.410, util loss 756063232.000\n",
      "Epoch: 4, iter 4000 loss: 452335040.0, masked traj loss 0.556, judge traj loss 0.296, util loss 452335040.000\n",
      "Epoch: 4, iter 4500 loss: 377194592.0, masked traj loss 0.266, judge traj loss 0.333, util loss 377194592.000\n",
      "Epoch: 4, iter 5000 loss: 609261696.0, masked traj loss 0.670, judge traj loss 0.307, util loss 609261696.000\n",
      "Epoch: 4, iter 5500 loss: 274595904.0, masked traj loss 0.518, judge traj loss 0.303, util loss 274595904.000\n",
      "Epoch: 4, iter 6000 loss: 613770432.0, masked traj loss 0.598, judge traj loss 0.382, util loss 613770432.000\n",
      "Epoch: 4, iter 6500 loss: 323720064.0, masked traj loss 0.570, judge traj loss 0.303, util loss 323720064.000\n",
      "Epoch: 4, iter 7000 loss: 547379328.0, masked traj loss 0.581, judge traj loss 0.261, util loss 547379328.000\n",
      "Epoch: 4, iter 7500 loss: 513712064.0, masked traj loss 0.711, judge traj loss 0.490, util loss 513712064.000\n",
      "Epoch: 4, iter 8000 loss: 302387744.0, masked traj loss 0.495, judge traj loss 0.233, util loss 302387744.000\n",
      "Epoch: 4, iter 8500 loss: 304859488.0, masked traj loss 0.336, judge traj loss 0.285, util loss 304859488.000\n",
      "Epoch: 4, iter 9000 loss: 414760128.0, masked traj loss 0.515, judge traj loss 0.297, util loss 414760128.000\n",
      "Epoch: 4, iter 9500 loss: 545104256.0, masked traj loss 0.542, judge traj loss 0.180, util loss 545104256.000\n",
      "Epoch: 4, iter 10000 loss: 224338816.0, masked traj loss 0.810, judge traj loss 0.641, util loss 224338816.000\n",
      "Epoch: 4, iter 10500 loss: 720988928.0, masked traj loss 0.580, judge traj loss 0.405, util loss 720988928.000\n",
      "Epoch 4 avg_loss=416641612.4293289 total_acc= 84.84\n",
      "Epoch: 5, iter 0 loss: 509334336.0, masked traj loss 0.477, judge traj loss 0.302, util loss 509334336.000\n",
      "Epoch: 5, iter 500 loss: 482959040.0, masked traj loss 0.626, judge traj loss 0.220, util loss 482959040.000\n",
      "Epoch: 5, iter 1000 loss: 413962784.0, masked traj loss 0.807, judge traj loss 0.362, util loss 413962784.000\n",
      "Epoch: 5, iter 1500 loss: 405782784.0, masked traj loss 0.517, judge traj loss 0.345, util loss 405782784.000\n",
      "Epoch: 5, iter 2000 loss: 232366592.0, masked traj loss 0.487, judge traj loss 0.209, util loss 232366592.000\n",
      "Epoch: 5, iter 2500 loss: 716252928.0, masked traj loss 0.574, judge traj loss 0.216, util loss 716252928.000\n",
      "Epoch: 5, iter 3000 loss: 299994592.0, masked traj loss 0.700, judge traj loss 0.261, util loss 299994592.000\n",
      "Epoch: 5, iter 3500 loss: 416656064.0, masked traj loss 0.701, judge traj loss 0.431, util loss 416656064.000\n",
      "Epoch: 5, iter 4000 loss: 452257216.0, masked traj loss 0.434, judge traj loss 0.358, util loss 452257216.000\n",
      "Epoch: 5, iter 4500 loss: 359650528.0, masked traj loss 0.527, judge traj loss 0.212, util loss 359650528.000\n",
      "Epoch: 5, iter 5000 loss: 420052512.0, masked traj loss 0.465, judge traj loss 0.249, util loss 420052512.000\n",
      "Epoch: 5, iter 5500 loss: 458681632.0, masked traj loss 0.335, judge traj loss 0.346, util loss 458681632.000\n",
      "Epoch: 5, iter 6000 loss: 634490496.0, masked traj loss 0.533, judge traj loss 0.450, util loss 634490496.000\n",
      "Epoch: 5, iter 6500 loss: 327008192.0, masked traj loss 0.511, judge traj loss 0.387, util loss 327008192.000\n",
      "Epoch: 5, iter 7000 loss: 414753984.0, masked traj loss 0.384, judge traj loss 0.303, util loss 414753984.000\n",
      "Epoch: 5, iter 7500 loss: 498734496.0, masked traj loss 0.561, judge traj loss 0.194, util loss 498734496.000\n",
      "Epoch: 5, iter 8000 loss: 329129984.0, masked traj loss 0.535, judge traj loss 0.227, util loss 329129984.000\n",
      "Epoch: 5, iter 8500 loss: 417067232.0, masked traj loss 0.378, judge traj loss 0.317, util loss 417067232.000\n",
      "Epoch: 5, iter 9000 loss: 246263136.0, masked traj loss 0.496, judge traj loss 0.386, util loss 246263136.000\n",
      "Epoch: 5, iter 9500 loss: 215262544.0, masked traj loss 0.543, judge traj loss 0.471, util loss 215262544.000\n",
      "Epoch: 5, iter 10000 loss: 638884352.0, masked traj loss 0.955, judge traj loss 0.337, util loss 638884352.000\n",
      "Epoch: 5, iter 10500 loss: 214788000.0, masked traj loss 0.449, judge traj loss 0.445, util loss 214788000.000\n",
      "Epoch 5 avg_loss=380625614.2256354 total_acc= 85.09457142857143\n",
      "Epoch: 6, iter 0 loss: 341405472.0, masked traj loss 0.754, judge traj loss 0.382, util loss 341405472.000\n",
      "Epoch: 6, iter 500 loss: 213578400.0, masked traj loss 0.657, judge traj loss 0.424, util loss 213578400.000\n",
      "Epoch: 6, iter 1000 loss: 588828928.0, masked traj loss 0.769, judge traj loss 0.317, util loss 588828928.000\n",
      "Epoch: 6, iter 1500 loss: 719266048.0, masked traj loss 0.634, judge traj loss 0.352, util loss 719266048.000\n",
      "Epoch: 6, iter 2000 loss: 552944384.0, masked traj loss 0.539, judge traj loss 0.382, util loss 552944384.000\n",
      "Epoch: 6, iter 2500 loss: 326465504.0, masked traj loss 0.333, judge traj loss 0.272, util loss 326465504.000\n",
      "Epoch: 6, iter 3000 loss: 475695136.0, masked traj loss 0.716, judge traj loss 0.463, util loss 475695136.000\n",
      "Epoch: 6, iter 3500 loss: 248017312.0, masked traj loss 0.382, judge traj loss 0.339, util loss 248017312.000\n",
      "Epoch: 6, iter 4000 loss: 211331536.0, masked traj loss 0.545, judge traj loss 0.349, util loss 211331536.000\n",
      "Epoch: 6, iter 4500 loss: 375123392.0, masked traj loss 0.290, judge traj loss 0.404, util loss 375123392.000\n",
      "Epoch: 6, iter 5000 loss: 335090816.0, masked traj loss 0.779, judge traj loss 0.321, util loss 335090816.000\n",
      "Epoch: 6, iter 5500 loss: 543213824.0, masked traj loss 0.299, judge traj loss 0.232, util loss 543213824.000\n",
      "Epoch: 6, iter 6000 loss: 282247776.0, masked traj loss 0.533, judge traj loss 0.144, util loss 282247776.000\n",
      "Epoch: 6, iter 6500 loss: 176225664.0, masked traj loss 0.752, judge traj loss 0.408, util loss 176225664.000\n",
      "Epoch: 6, iter 7000 loss: 431870272.0, masked traj loss 0.727, judge traj loss 0.282, util loss 431870272.000\n",
      "Epoch: 6, iter 7500 loss: 222792720.0, masked traj loss 0.297, judge traj loss 0.352, util loss 222792720.000\n",
      "Epoch: 6, iter 8000 loss: 428621568.0, masked traj loss 0.435, judge traj loss 0.332, util loss 428621568.000\n",
      "Epoch: 6, iter 8500 loss: 313194976.0, masked traj loss 0.591, judge traj loss 0.190, util loss 313194976.000\n",
      "Epoch: 6, iter 9000 loss: 344322720.0, masked traj loss 0.701, judge traj loss 0.237, util loss 344322720.000\n",
      "Epoch: 6, iter 9500 loss: 224094096.0, masked traj loss 0.699, judge traj loss 0.521, util loss 224094096.000\n",
      "Epoch: 6, iter 10000 loss: 354049664.0, masked traj loss 0.648, judge traj loss 0.390, util loss 354049664.000\n",
      "Epoch: 6, iter 10500 loss: 312727872.0, masked traj loss 0.659, judge traj loss 0.215, util loss 312727872.000\n",
      "Epoch 6 avg_loss=350733878.0515634 total_acc= 85.09571428571428\n",
      "Epoch: 7, iter 0 loss: 234168272.0, masked traj loss 0.677, judge traj loss 0.358, util loss 234168272.000\n",
      "Epoch: 7, iter 500 loss: 216288432.0, masked traj loss 0.670, judge traj loss 0.310, util loss 216288432.000\n",
      "Epoch: 7, iter 1000 loss: 350932448.0, masked traj loss 0.435, judge traj loss 0.487, util loss 350932448.000\n",
      "Epoch: 7, iter 1500 loss: 454135072.0, masked traj loss 0.527, judge traj loss 0.189, util loss 454135072.000\n",
      "Epoch: 7, iter 2000 loss: 292010752.0, masked traj loss 0.463, judge traj loss 0.348, util loss 292010752.000\n",
      "Epoch: 7, iter 2500 loss: 225287824.0, masked traj loss 0.585, judge traj loss 0.465, util loss 225287824.000\n",
      "Epoch: 7, iter 3000 loss: 393101472.0, masked traj loss 0.388, judge traj loss 0.246, util loss 393101472.000\n",
      "Epoch: 7, iter 3500 loss: 339884000.0, masked traj loss 0.622, judge traj loss 0.296, util loss 339884000.000\n",
      "Epoch: 7, iter 4000 loss: 317603104.0, masked traj loss 0.630, judge traj loss 0.332, util loss 317603104.000\n",
      "Epoch: 7, iter 4500 loss: 301477248.0, masked traj loss 0.332, judge traj loss 0.306, util loss 301477248.000\n",
      "Epoch: 7, iter 5000 loss: 279826208.0, masked traj loss 0.500, judge traj loss 0.433, util loss 279826208.000\n",
      "Epoch: 7, iter 5500 loss: 234068608.0, masked traj loss 0.490, judge traj loss 0.325, util loss 234068608.000\n",
      "Epoch: 7, iter 6000 loss: 223723264.0, masked traj loss 0.453, judge traj loss 0.382, util loss 223723264.000\n",
      "Epoch: 7, iter 6500 loss: 171110128.0, masked traj loss 0.349, judge traj loss 0.402, util loss 171110128.000\n",
      "Epoch: 7, iter 7000 loss: 633904448.0, masked traj loss 0.677, judge traj loss 0.260, util loss 633904448.000\n",
      "Epoch: 7, iter 7500 loss: 340072640.0, masked traj loss 0.476, judge traj loss 0.378, util loss 340072640.000\n",
      "Epoch: 7, iter 8000 loss: 468775744.0, masked traj loss 0.616, judge traj loss 0.483, util loss 468775744.000\n",
      "Epoch: 7, iter 8500 loss: 244946592.0, masked traj loss 0.454, judge traj loss 0.302, util loss 244946592.000\n",
      "Epoch: 7, iter 9000 loss: 337750688.0, masked traj loss 0.616, judge traj loss 0.416, util loss 337750688.000\n",
      "Epoch: 7, iter 9500 loss: 199927376.0, masked traj loss 0.878, judge traj loss 0.375, util loss 199927376.000\n",
      "Epoch: 7, iter 10000 loss: 372628544.0, masked traj loss 0.523, judge traj loss 0.215, util loss 372628544.000\n",
      "Epoch: 7, iter 10500 loss: 337290688.0, masked traj loss 0.782, judge traj loss 0.526, util loss 337290688.000\n",
      "Epoch 7 avg_loss=323074162.85207534 total_acc= 84.994\n",
      "Epoch: 8, iter 0 loss: 251346080.0, masked traj loss 0.646, judge traj loss 0.448, util loss 251346080.000\n",
      "Epoch: 8, iter 500 loss: 318489504.0, masked traj loss 0.538, judge traj loss 0.498, util loss 318489504.000\n",
      "Epoch: 8, iter 1000 loss: 426641120.0, masked traj loss 0.821, judge traj loss 0.527, util loss 426641120.000\n",
      "Epoch: 8, iter 1500 loss: 344155136.0, masked traj loss 0.540, judge traj loss 0.267, util loss 344155136.000\n",
      "Epoch: 8, iter 2000 loss: 476386592.0, masked traj loss 0.633, judge traj loss 0.324, util loss 476386592.000\n",
      "Epoch: 8, iter 2500 loss: 403762816.0, masked traj loss 0.562, judge traj loss 0.215, util loss 403762816.000\n",
      "Epoch: 8, iter 3000 loss: 352797824.0, masked traj loss 0.362, judge traj loss 0.311, util loss 352797824.000\n",
      "Epoch: 8, iter 3500 loss: 238406896.0, masked traj loss 0.579, judge traj loss 0.335, util loss 238406896.000\n",
      "Epoch: 8, iter 4000 loss: 455544992.0, masked traj loss 0.227, judge traj loss 0.424, util loss 455544992.000\n",
      "Epoch: 8, iter 4500 loss: 305285824.0, masked traj loss 0.313, judge traj loss 0.285, util loss 305285824.000\n",
      "Epoch: 8, iter 5000 loss: 314928224.0, masked traj loss 0.556, judge traj loss 0.277, util loss 314928224.000\n",
      "Epoch: 8, iter 5500 loss: 139551920.0, masked traj loss 0.692, judge traj loss 0.571, util loss 139551920.000\n",
      "Epoch: 8, iter 6000 loss: 214705952.0, masked traj loss 0.340, judge traj loss 0.358, util loss 214705952.000\n",
      "Epoch: 8, iter 6500 loss: 200019584.0, masked traj loss 0.734, judge traj loss 0.486, util loss 200019584.000\n",
      "Epoch: 8, iter 7000 loss: 368760320.0, masked traj loss 0.407, judge traj loss 0.467, util loss 368760320.000\n",
      "Epoch: 8, iter 7500 loss: 264342640.0, masked traj loss 0.506, judge traj loss 0.498, util loss 264342640.000\n",
      "Epoch: 8, iter 8000 loss: 314746368.0, masked traj loss 0.565, judge traj loss 0.333, util loss 314746368.000\n",
      "Epoch: 8, iter 8500 loss: 210362592.0, masked traj loss 0.324, judge traj loss 0.367, util loss 210362592.000\n",
      "Epoch: 8, iter 9000 loss: 276381440.0, masked traj loss 0.519, judge traj loss 0.427, util loss 276381440.000\n",
      "Epoch: 8, iter 9500 loss: 118098384.0, masked traj loss 0.497, judge traj loss 0.275, util loss 118098384.000\n",
      "Epoch: 8, iter 10000 loss: 220504192.0, masked traj loss 0.730, judge traj loss 0.244, util loss 220504192.000\n",
      "Epoch: 8, iter 10500 loss: 365127008.0, masked traj loss 0.389, judge traj loss 0.087, util loss 365127008.000\n",
      "Epoch 8 avg_loss=298699686.0541232 total_acc= 85.54914285714285\n",
      "Epoch: 9, iter 0 loss: 319285728.0, masked traj loss 0.469, judge traj loss 0.314, util loss 319285728.000\n",
      "Epoch: 9, iter 500 loss: 174827856.0, masked traj loss 0.508, judge traj loss 0.282, util loss 174827856.000\n",
      "Epoch: 9, iter 1000 loss: 179813776.0, masked traj loss 0.600, judge traj loss 0.605, util loss 179813776.000\n",
      "Epoch: 9, iter 1500 loss: 120939960.0, masked traj loss 0.543, judge traj loss 0.326, util loss 120939960.000\n",
      "Epoch: 9, iter 2000 loss: 270406080.0, masked traj loss 0.736, judge traj loss 0.527, util loss 270406080.000\n",
      "Epoch: 9, iter 2500 loss: 261567664.0, masked traj loss 0.582, judge traj loss 0.329, util loss 261567664.000\n",
      "Epoch: 9, iter 3000 loss: 259155168.0, masked traj loss 0.216, judge traj loss 0.278, util loss 259155168.000\n",
      "Epoch: 9, iter 3500 loss: 285703328.0, masked traj loss 0.533, judge traj loss 0.263, util loss 285703328.000\n",
      "Epoch: 9, iter 4000 loss: 307951936.0, masked traj loss 0.258, judge traj loss 0.307, util loss 307951936.000\n",
      "Epoch: 9, iter 4500 loss: 162290416.0, masked traj loss 0.667, judge traj loss 0.256, util loss 162290416.000\n",
      "Epoch: 9, iter 5000 loss: 127788784.0, masked traj loss 0.399, judge traj loss 0.210, util loss 127788784.000\n",
      "Epoch: 9, iter 5500 loss: 144057792.0, masked traj loss 0.693, judge traj loss 0.605, util loss 144057792.000\n",
      "Epoch: 9, iter 6000 loss: 247051424.0, masked traj loss 0.483, judge traj loss 0.258, util loss 247051424.000\n",
      "Epoch: 9, iter 6500 loss: 417678080.0, masked traj loss 0.306, judge traj loss 0.294, util loss 417678080.000\n",
      "Epoch: 9, iter 7000 loss: 370410240.0, masked traj loss 0.493, judge traj loss 0.338, util loss 370410240.000\n",
      "Epoch: 9, iter 7500 loss: 251162336.0, masked traj loss 0.605, judge traj loss 0.354, util loss 251162336.000\n",
      "Epoch: 9, iter 8000 loss: 237661248.0, masked traj loss 0.638, judge traj loss 0.539, util loss 237661248.000\n",
      "Epoch: 9, iter 8500 loss: 143436960.0, masked traj loss 0.309, judge traj loss 0.569, util loss 143436960.000\n",
      "Epoch: 9, iter 9000 loss: 266157584.0, masked traj loss 0.572, judge traj loss 0.496, util loss 266157584.000\n",
      "Epoch: 9, iter 9500 loss: 307233024.0, masked traj loss 0.531, judge traj loss 0.187, util loss 307233024.000\n",
      "Epoch: 9, iter 10000 loss: 182420464.0, masked traj loss 0.411, judge traj loss 0.281, util loss 182420464.000\n",
      "Epoch: 9, iter 10500 loss: 229295872.0, masked traj loss 0.485, judge traj loss 0.278, util loss 229295872.000\n",
      "Epoch 9 avg_loss=279328016.8740172 total_acc= 85.49142857142857\n"
     ]
    }
   ],
   "source": [
    "model.train(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11331, 256)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = model.model.transformer.embed.tok_embed.weight.data.cpu().numpy()\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec_emb = node2vec.load_emb()\n",
    "gae_emb = gae.load_emb()\n",
    "srn_emb = srn.load_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model.state_dict(), os.path.join(\"../model_states/gtn/\" + \"/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3947719080530501\n",
      "0.42961658189556384\n",
      "0.4372306680973404\n",
      "0.44664834892379923\n",
      "0.25823137610760594\n",
      "0.20901425528334538\n",
      "0.06776537821165748\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "idxs = np.arange(len(network.line_graph.nodes))\n",
    "# train_idx, test_idx = model_selection.train_test_split(idxs, test_size=0.2, random_state=69)\n",
    "y = np.array([network.gdf_edges.loc[n][\"highway_enc\"] for n in network.line_graph.nodes])\n",
    "\n",
    "z = emb\n",
    "zct = np.concatenate((init_emb, emb), axis=1)\n",
    "zadd = np.add(emb, init_emb)\n",
    "# zcnn = np.concatenate((zn, z4), axis=1)\n",
    "# zctn = np.concatenate((zn, z5), axis=1)\n",
    "# X = z # embedding for each node\n",
    "eva = [z, zct, zadd, init_emb, gae_emb, node2vec_emb, srn_emb]\n",
    "for X in eva:\n",
    "    # X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    lm = linear_model.LogisticRegression(multi_class=\"multinomial\", max_iter=1000)\n",
    "    # lm.fit(X_train, y_train)\n",
    "    scorer = make_scorer(metrics.f1_score, average=\"macro\")\n",
    "    print(np.mean(cross_val_score(estimator=lm, X=X, y=y, scoring=scorer, cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.143396457691676\n",
      "28.72703877806518\n",
      "14.17213249946948\n",
      "23.887047557215617\n",
      "14.368647958034728\n",
      "15.343440512087856\n",
      "15.632968640714669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "tf = pd.read_csv(\"../../datasets/trajectories/Porto/speed_features_unnormalized.csv\")\n",
    "tf.set_index([\"u\", \"v\", \"key\"], inplace=True)\n",
    "map_id = {j: i for i, j in enumerate(network.line_graph.nodes)}\n",
    "tf[\"idx\"] = tf.index.map(map_id)\n",
    "tf.sort_values(by=\"idx\", axis=0, inplace=True)\n",
    "\n",
    "idxs = np.arange(len(network.line_graph.nodes))\n",
    "train_idx, test_idx = model_selection.train_test_split(idxs, test_size=0.2, random_state=69)\n",
    "\n",
    "y = tf[\"avg_speed\"]\n",
    "y.fillna(0, inplace=True)\n",
    "y = y.round(2)\n",
    "y = y.values\n",
    "\n",
    "z = emb\n",
    "zct = np.concatenate((init_emb, emb), axis=1)\n",
    "zadd = np.add(emb, init_emb)\n",
    "eva = [z, zct, zadd, init_emb, gae_emb, node2vec_emb, srn_emb]\n",
    "for X in eva:\n",
    "    decoder = linear_model.LinearRegression(fit_intercept=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    # decoder.fit(X_train, y_train)\n",
    "    scorer = make_scorer(metrics.mean_absolute_error)\n",
    "    print(np.mean(cross_val_score(estimator=decoder, X=X, y=y, scoring=scorer, cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 12122.260076701406, 'MAE': 76.50991675745647}\n",
      "{'MSE': 11803.74874593377, 'MAE': 76.66678889745077}\n",
      "{'MSE': 11996.927491581646, 'MAE': 78.08713494930268}\n",
      "{'MSE': 11954.191135516034, 'MAE': 76.86543172111512}\n",
      "{'MSE': 12889.226716738953, 'MAE': 80.59984841804504}\n",
      "{'MSE': 12825.068171002236, 'MAE': 81.41830803044637}\n",
      "{'MSE': 11932.491779679805, 'MAE': 77.10150223821005}\n"
     ]
    }
   ],
   "source": [
    "travel_time_est = TravelTimeEstimation(\n",
    "    traj_dataset=test,\n",
    "    network=network,\n",
    "    device=device,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    seed=88,\n",
    ")\n",
    "travel_time_est.register_metric(\n",
    "    name=\"MSE\", metric_func=metrics.mean_squared_error, args={}\n",
    ")\n",
    "travel_time_est.register_metric(\n",
    "    name=\"MAE\", metric_func=metrics.mean_absolute_error, args={}\n",
    ")\n",
    "\n",
    "z = emb\n",
    "zct = np.concatenate((init_emb, emb), axis=1)\n",
    "zadd = np.add(emb, init_emb)\n",
    "eva = [z, zct, zadd, init_emb, gae_emb, node2vec_emb, srn_emb]\n",
    "for X in eva:\n",
    "    print(travel_time_est.evaluate(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8b21b806be449a935fd3bd3dfe86ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987b9602d51b4973a62c930560fced95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316c1d9109734109b181d5225f48ff5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1206d5466c0849dd9a7e6be2543584d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 0: 94.48265817475472\n",
      "Average training loss in episode 1: 57.049108859064226\n",
      "Average training loss in episode 2: 38.65703039840341\n",
      "Average training loss in episode 3: 28.943796719061034\n",
      "Average training loss in episode 4: 22.36021076031585\n",
      "{'accuracy': 0.5639333333333333}\n",
      "Average training loss in episode 0: 92.04217865217977\n",
      "Average training loss in episode 1: 53.717257957214485\n",
      "Average training loss in episode 2: 36.37113255173413\n",
      "Average training loss in episode 3: 26.49525107605371\n",
      "Average training loss in episode 4: 20.53188714594729\n",
      "{'accuracy': 0.6016}\n",
      "Average training loss in episode 0: 91.76211930134657\n",
      "Average training loss in episode 1: 55.27608842178702\n",
      "Average training loss in episode 2: 37.71371385537739\n",
      "Average training loss in episode 3: 27.420851697291393\n",
      "Average training loss in episode 4: 21.131240116761944\n",
      "{'accuracy': 0.5858333333333333}\n",
      "Average training loss in episode 0: 98.54018650990305\n",
      "Average training loss in episode 1: 68.48151796255539\n",
      "Average training loss in episode 2: 48.65221245609113\n",
      "Average training loss in episode 3: 37.39505357884649\n",
      "Average training loss in episode 4: 29.526176814585607\n",
      "{'accuracy': 0.5341}\n",
      "Average training loss in episode 0: 102.97498206034906\n",
      "Average training loss in episode 1: 79.00903417103326\n",
      "Average training loss in episode 2: 60.98999041331602\n",
      "Average training loss in episode 3: 48.94844979162155\n",
      "Average training loss in episode 4: 40.83822947422833\n",
      "{'accuracy': 0.4602333333333333}\n",
      "Average training loss in episode 0: 103.354843920482\n",
      "Average training loss in episode 1: 82.54645539956815\n",
      "Average training loss in episode 2: 56.66776765282474\n",
      "Average training loss in episode 3: 38.74741872693938\n",
      "Average training loss in episode 4: 28.951475753458833\n",
      "{'accuracy': 0.5304666666666666}\n",
      "Average training loss in episode 0: 90.78991051777594\n",
      "Average training loss in episode 1: 55.90766653780744\n",
      "Average training loss in episode 2: 40.522590271191305\n",
      "Average training loss in episode 3: 31.11748547747191\n",
      "Average training loss in episode 4: 24.314490391501486\n",
      "{'accuracy': 0.5480666666666667}\n"
     ]
    }
   ],
   "source": [
    "nextlocation_pred = NextLocationPrediciton(\n",
    "    traj_dataset=test,\n",
    "    network=network,\n",
    "    device=device,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    seed=88,\n",
    ")\n",
    "\n",
    "nextlocation_pred.register_metric(\n",
    "    name=\"accuracy\",\n",
    "    metric_func=metrics.accuracy_score,\n",
    "    args={\"normalize\": True},\n",
    ")\n",
    "\n",
    "z = emb\n",
    "zctn = np.concatenate((init_emb, emb), axis=1)\n",
    "zadd = np.add(emb, init_emb)\n",
    "eva = [z, zct, zadd, init_emb, gae_emb, node2vec_emb, srn_emb]\n",
    "for X in eva:\n",
    "    print(nextlocation_pred.evaluate(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af53f7afe614ada95bf6d0f2d9a26c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2efbc92283d4b57b2c40d684d0cdf3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/120000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ee8d703cbe4560bf5d2290d03646f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafb8db7f49a49a69a97a17c4dd81169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss in episode 0: 6.330635722512121\n",
      "Average training loss in episode 1: 4.574502902752809\n",
      "Average training loss in episode 2: 3.984619103006717\n",
      "Average training loss in episode 3: 3.6476753185044473\n",
      "Average training loss in episode 4: 3.4101818782180104\n",
      "{'accuracy': 0.23533333333333334}\n",
      "Average training loss in episode 0: 6.174136269575497\n",
      "Average training loss in episode 1: 4.450191152629568\n",
      "Average training loss in episode 2: 3.8868592908895856\n",
      "Average training loss in episode 3: 3.562092793775774\n",
      "Average training loss in episode 4: 3.332198644751933\n",
      "{'accuracy': 0.2413}\n",
      "Average training loss in episode 0: 6.1727608353344365\n",
      "Average training loss in episode 1: 4.460918168777596\n",
      "Average training loss in episode 2: 3.8801761995246418\n",
      "Average training loss in episode 3: 3.537017314927156\n",
      "Average training loss in episode 4: 3.28957672058138\n",
      "{'accuracy': 0.24276666666666666}\n",
      "Average training loss in episode 0: 6.486407523978748\n",
      "Average training loss in episode 1: 4.668549554942768\n",
      "Average training loss in episode 2: 4.015694802249673\n",
      "Average training loss in episode 3: 3.6498418342330052\n",
      "Average training loss in episode 4: 3.3899615629395443\n",
      "{'accuracy': 0.2409}\n",
      "Average training loss in episode 0: 7.291179833890024\n",
      "Average training loss in episode 1: 5.595992958621938\n",
      "Average training loss in episode 2: 4.687317386619063\n",
      "Average training loss in episode 3: 4.152681875838908\n",
      "Average training loss in episode 4: 3.7805306489533708\n",
      "{'accuracy': 0.21606666666666666}\n",
      "Average training loss in episode 0: 6.695423094702682\n",
      "Average training loss in episode 1: 4.881414859787996\n",
      "Average training loss in episode 2: 4.1985228214182575\n",
      "Average training loss in episode 3: 3.8066769637532833\n",
      "Average training loss in episode 4: 3.5313893752311594\n",
      "{'accuracy': 0.23}\n",
      "Average training loss in episode 0: 7.271148304441082\n",
      "Average training loss in episode 1: 5.3617633410862515\n",
      "Average training loss in episode 2: 4.425199395812142\n",
      "Average training loss in episode 3: 3.895098264283463\n",
      "Average training loss in episode 4: 3.5267312257274637\n",
      "{'accuracy': 0.2357}\n"
     ]
    }
   ],
   "source": [
    "dest_pred = DestinationPrediciton(\n",
    "    traj_dataset=test,\n",
    "    network=network,\n",
    "    device=device,\n",
    "    batch_size=256,\n",
    "    epochs=5,\n",
    "    seed=88,\n",
    ")\n",
    "\n",
    "dest_pred.register_metric(\n",
    "    name=\"accuracy\",\n",
    "    metric_func=metrics.accuracy_score,\n",
    "    args={\"normalize\": True},\n",
    ")\n",
    "\n",
    "z = emb\n",
    "zct = np.concatenate((init_emb, emb), axis=1)\n",
    "zadd = np.add(emb, init_emb)\n",
    "eva = [z, zct, zadd, init_emb, gae_emb, node2vec_emb, srn_emb]\n",
    "for X in eva:\n",
    "    print(dest_pred.evaluate(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('road')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "088070de2c6b4023b2f7ae556c412f86bcd02589c7bdb3766a0caf3cf4813fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
